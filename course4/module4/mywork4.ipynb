{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4291585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a56f4",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "Application\n",
    "\n",
    "* Grouping similar words into semantic concepts\n",
    "* As a building block in natural language understanding tasks\n",
    "  * Textual entailment : Derive meaning of sentence from another sentences\n",
    "  * Paraphrasing : Make sentence with same meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4268664",
   "metadata": {},
   "source": [
    "<Q> Is there a weight between words in word net?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727db1a",
   "metadata": {},
   "source": [
    "[Using wordnet by nltk](https://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff9039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb583baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d24316",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = wn.synset('catch.v.02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d0642",
   "metadata": {},
   "source": [
    "### Sortest path similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab14f013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(elk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcb70c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(catch) #되네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db555533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'acyclic_tree',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'in_region_domains',\n",
       " 'in_topic_domains',\n",
       " 'in_usage_domains',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'mst',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(deer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa47ce",
   "metadata": {},
   "source": [
    "### Lin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a4d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic # ic is information content\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d5c07",
   "metadata": {},
   "source": [
    "In information theory, the **information content, self-information, surprisal, or Shannon information** is a basic quantity derived from the probability of a particular event occurring from a random variable. It can be thought of as an alternative way of expressing probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651b9e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n', 'v'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_ic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b765d045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623778273893673"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 * IC(lcs) / (IC(s1) + IC(s2))\n",
    "\n",
    "deer.lin_similarity(elk,brown_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebe7328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "WordNetError",
     "evalue": "Computing the least common subsumer requires Synset('deer.n.01') and Synset('catch.v.02') to have the same part of speech.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4895448aea77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbrown_ic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mlin_similarity\u001b[1;34m(self, other, ic, verbose)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \"\"\"\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[0mic1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mic2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlcs_ic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lcs_ic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlcs_ic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mic1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mic2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_lcs_ic\u001b[1;34m(synset1, synset2, ic, verbose)\u001b[0m\n\u001b[0;32m   2144\u001b[0m     \"\"\"\n\u001b[0;32m   2145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msynset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msynset2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m         raise WordNetError(\n\u001b[0m\u001b[0;32m   2147\u001b[0m             \u001b[1;34m\"Computing the least common subsumer requires \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m             \u001b[1;34m\"%s and %s to have the same part of speech.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msynset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWordNetError\u001b[0m: Computing the least common subsumer requires Synset('deer.n.01') and Synset('catch.v.02') to have the same part of speech."
     ]
    }
   ],
   "source": [
    "deer.lin_similarity(catch,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bd228",
   "metadata": {},
   "source": [
    "<Q> How can I calculate sentence similarity?\n",
    "    \n",
    "I think we should start with word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fae0b",
   "metadata": {},
   "source": [
    "### Collocation & Distribution similarity\n",
    "\n",
    "솔직히 아직 잘 모르겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce25215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "finder.nbest(bigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc433126",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "**Documents are mixture of topics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf078af8",
   "metadata": {},
   "source": [
    "Though it is likely there as well, right? So for a particular word, you have different distribution or probable occurring from a topic, and topics are basically this probability of distribution over all words. A document is assumed to be a mixture of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbb68d",
   "metadata": {},
   "source": [
    "And then you also need to figure out what documents come together. What documents are of the same topic or mostly about the same topic?\n",
    "And how does these words get derived based on these documents? So how do you build this topic modeling to understand what is a distribution of words in a particular document and what is a probability of a word in a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2befb",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715792d",
   "metadata": {},
   "source": [
    "**1. Preprocessing is neccessary**\n",
    "\n",
    " 1. Tokenize, normalize(lowercase)\n",
    " 2. Stop word removal - Common words that occur frequently, but don't help analysis\n",
    " 3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227323a",
   "metadata": {},
   "source": [
    "Stopword example\n",
    "\n",
    "* I, I'm ...\n",
    "* The word 'patient' in medical document becuase It appears too frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f110d72",
   "metadata": {},
   "source": [
    " **2. Convert Tokenized documents to a document - term matrix(So going from which document has what words to what words are occurring in which documents)**\n",
    " \n",
    " **3. Build LDA models on the doc-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34305b",
   "metadata": {},
   "source": [
    "[LDA in python](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#10removestopwordsmakebigramsandlemmatize)\n",
    "\n",
    "[LDA WIKI](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model)\n",
    "\n",
    "[Plate Notation](https://en.wikipedia.org/wiki/Plate_notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA 모델 만들어보기\n",
    "#LDA 좀 이해해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4b2fa",
   "metadata": {},
   "source": [
    "# Information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7efc7",
   "metadata": {},
   "source": [
    "### Information is hidden in free text\n",
    "\n",
    "- Most traditional transactional infromation is structured\n",
    "- Now unstructured freeform text is abundanced\n",
    "\n",
    "**-> How to convert unstructured text to structured form? -> How to extract relevant information from unstructured text?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70652829",
   "metadata": {},
   "source": [
    "### Information extraction\n",
    "\n",
    "**Goal : Indentify and extract fields of interest from free text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66698547",
   "metadata": {},
   "source": [
    "So for example, bilateral hand numbness is a completely valid named entity in itself. But hand is also a named entity that is of interest. So then the question becomes, what is the granularity with which you need to annotate this.\n",
    "\n",
    "\n",
    "And if you're doing hand as a separate annotation, why not C5-6 disc or cord because you're talking about spinal cord in this case, you're talking about an anatomical part.\n",
    "\n",
    "\n",
    "So these are the decisions that need to be made when you're defining this named entity task. So in this particular case, if you define the task to be one, where you say we are only interested in the condition, the diagnosis, the age, the gender, the procedures that were done, and the specialty. That's it. In this then, you're not going to focus on a body part, hand and feet, and even if they are there, that's fine. And some decision already was done that way. So for example, nobody said that patient is not a valid entity in this role, it could be, it very well could be. If neurologist is a valid entity, patient could be a valid entity, and that's a decision that has to be made when you're defining this named entity task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76cac8",
   "metadata": {},
   "source": [
    "**Question** : Can I use graph theory to analyze text?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ec26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
