{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "In this assignment you will explore text message data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "What percentage of the documents in `spam_data` are spam?\n",
    "\n",
    "*This function should return a float, the percent value (i.e. $ratio * 100$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    spam = len(spam_data[spam_data['target']==1])\n",
    "    total = spam_data.shape[0]\n",
    "    return (spam/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Fit the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "What is the longest token in the vocabulary?\n",
    "\n",
    "*This function should return a string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def answer_two():\n",
    "    cv = CountVectorizer().fit(X_train)\n",
    "    len_lst = [len(x) for x in cv.get_feature_names()]\n",
    "    \n",
    "    return cv.get_feature_names()[np.argmax(len_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def answer_three():\n",
    "    cv = CountVectorizer().fit(X_train)\n",
    "    X_train_vectorized = cv.transform(X_train)\n",
    "    X_test_vectorized = cv.transform(X_test)\n",
    "    \n",
    "    model = MultinomialNB(alpha=0.1).fit(X_train_vectorized,y_train)\n",
    "    prediction = model.predict(X_test_vectorized)\n",
    "    \n",
    "    return roc_auc_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n",
    "\n",
    "What 20 features have the smallest tf-idf and what 20 have the largest tf-idf?\n",
    "\n",
    "Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series should be the feature name, and the data should be the tf-idf.\n",
    "\n",
    "The series of 20 features with smallest tf-idfs should be sorted smallest tfidf first, the list of 20 features with largest tf-idfs should be sorted largest first. \n",
    "\n",
    "*This function should return a tuple of two series\n",
    "`(smallest tf-idfs series, largest tf-idfs series)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect_tfidf = TfidfVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect_tfidf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7354 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55130 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7305, 0.06632067425788671),\n",
       " (7042, 0.14330016183230146),\n",
       " (6588, 0.37747796780025566),\n",
       " (6587, 0.17282609864122322),\n",
       " (6407, 0.2059448631921737),\n",
       " (6143, 0.26739572828955477),\n",
       " (5658, 0.1329629555898702),\n",
       " (5400, 0.2163855662539368),\n",
       " (4407, 0.09458014299826566),\n",
       " (4276, 0.2530568446352961),\n",
       " (4218, 0.2530568446352961),\n",
       " (3348, 0.2530568446352961),\n",
       " (2928, 0.2530568446352961),\n",
       " (2887, 0.22623488920411522),\n",
       " (2782, 0.09387164742520636),\n",
       " (2472, 0.17378593282458957),\n",
       " (1929, 0.17477830989058163),\n",
       " (1811, 0.1588328817490513),\n",
       " (1768, 0.3965297050210666),\n",
       " (1541, 0.10377192307738273),\n",
       " (1500, 0.13369786414477738),\n",
       " (491, 0.16536481116829066)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = 6\n",
    "feature_index = X_train_vectorized[doc,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [X_train_vectorized[doc, x] for x in feature_index])\n",
    "list(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0, ..., 4178, 4178, 4178], dtype=int32),\n",
       " array([7305, 7085, 6421, ..., 2065,  931,  790], dtype=int32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero = X_train_vectorized.nonzero()\n",
    "nonzero #첫번째가 row(문장) , 두번째가 column(단어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7305</td>\n",
       "      <td>0.198151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7085</td>\n",
       "      <td>0.352010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6421</td>\n",
       "      <td>0.390988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4624</td>\n",
       "      <td>0.471125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3933</td>\n",
       "      <td>0.354634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55125</th>\n",
       "      <td>3336</td>\n",
       "      <td>0.179845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55126</th>\n",
       "      <td>3199</td>\n",
       "      <td>0.275090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55127</th>\n",
       "      <td>2065</td>\n",
       "      <td>0.347443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55128</th>\n",
       "      <td>931</td>\n",
       "      <td>0.241310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55129</th>\n",
       "      <td>790</td>\n",
       "      <td>0.225107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     tfidf\n",
       "0       7305  0.198151\n",
       "1       7085  0.352010\n",
       "2       6421  0.390988\n",
       "3       4624  0.471125\n",
       "4       3933  0.354634\n",
       "...      ...       ...\n",
       "55125   3336  0.179845\n",
       "55126   3199  0.275090\n",
       "55127   2065  0.347443\n",
       "55128    931  0.241310\n",
       "55129    790  0.225107\n",
       "\n",
       "[55130 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_tfidf = [X_train_vectorized[nonzero[0][idx],nonzero[1][idx]] for idx in range(len(nonzero[0]))]\n",
    "nonzero_tfidf_df = pd.DataFrame(nonzero_tfidf,index=nonzero[1],columns=['tfidf'])\n",
    "nonzero_tfidf_df = nonzero_tfidf_df.reset_index()\n",
    "def changeIdxWord(item):\n",
    "    return vect_tfidf.get_feature_names()[item]\n",
    "nonzero_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '000pes',\n",
       " '008704050406',\n",
       " '0089',\n",
       " '0121',\n",
       " '01223585236',\n",
       " '01223585334',\n",
       " '0125698789',\n",
       " '02',\n",
       " '0207',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '021',\n",
       " '03',\n",
       " '04',\n",
       " '0430',\n",
       " '05',\n",
       " '0578',\n",
       " '06',\n",
       " '07008009200',\n",
       " '07099833605',\n",
       " '07123456789',\n",
       " '0721072',\n",
       " '07734396839',\n",
       " '07742676969',\n",
       " '07753741225',\n",
       " '0776xxxxxxx',\n",
       " '07781482378',\n",
       " '07786200117',\n",
       " '077xxx',\n",
       " '078',\n",
       " '07808247860',\n",
       " '07808726822',\n",
       " '07821230901',\n",
       " '078498',\n",
       " '07880867867',\n",
       " '0789xxxxxxx',\n",
       " '07946746291',\n",
       " '0796xxxxxx',\n",
       " '07973788240',\n",
       " '07xxxxxxxxx',\n",
       " '08',\n",
       " '0800',\n",
       " '08000407165',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '08000938767',\n",
       " '08001950382',\n",
       " '08002888812',\n",
       " '08002986030',\n",
       " '08002986906',\n",
       " '08006344447',\n",
       " '0808',\n",
       " '08081263000',\n",
       " '08081560665',\n",
       " '0825',\n",
       " '0844',\n",
       " '08448350055',\n",
       " '08448714184',\n",
       " '0845',\n",
       " '08452810071',\n",
       " '08452810073',\n",
       " '08452810075over18',\n",
       " '0870',\n",
       " '08700435505150p',\n",
       " '08700469649',\n",
       " '08700621170150p',\n",
       " '08701213186',\n",
       " '08701237397',\n",
       " '08701417012',\n",
       " '08701417012150p',\n",
       " '087016248',\n",
       " '08701752560',\n",
       " '087018728737',\n",
       " '0870241182716',\n",
       " '08702840625',\n",
       " '08704050406',\n",
       " '08704439680',\n",
       " '08704439680ts',\n",
       " '08706091795',\n",
       " '08707509020',\n",
       " '08707808226',\n",
       " '08708034412',\n",
       " '08708800282',\n",
       " '08709222922',\n",
       " '0871',\n",
       " '087104711148',\n",
       " '08712101358',\n",
       " '08712103738',\n",
       " '0871212025016',\n",
       " '08712300220',\n",
       " '087123002209am',\n",
       " '08712317606',\n",
       " '08712400602450p',\n",
       " '08712400603',\n",
       " '08712402050',\n",
       " '08712402578',\n",
       " '08712402779',\n",
       " '08712402902',\n",
       " '08712404000',\n",
       " '08712405020',\n",
       " '08712405022',\n",
       " '08712460324',\n",
       " '08712466669',\n",
       " '0871277810710p',\n",
       " '0871277810910p',\n",
       " '08714712379',\n",
       " '08714712388',\n",
       " '08714714011',\n",
       " '08715203649',\n",
       " '08715203677',\n",
       " '08715203694',\n",
       " '08715205273',\n",
       " '08715500022',\n",
       " '08715705022',\n",
       " '08717111821',\n",
       " '08717168528',\n",
       " '08717205546',\n",
       " '0871750',\n",
       " '08717507382',\n",
       " '08717890890å',\n",
       " '08717895698',\n",
       " '08717898035',\n",
       " '08718711108',\n",
       " '08718720201',\n",
       " '08718725756',\n",
       " '08718726270',\n",
       " '087187262701',\n",
       " '08718726978',\n",
       " '087187272008',\n",
       " '08718727868',\n",
       " '08718727870',\n",
       " '08718730666',\n",
       " '08718738001',\n",
       " '08718738002',\n",
       " '08719180219',\n",
       " '08719180248',\n",
       " '08719181259',\n",
       " '08719181503',\n",
       " '08719839835',\n",
       " '08719899229',\n",
       " '09',\n",
       " '09041940223',\n",
       " '09050000301',\n",
       " '09050000332',\n",
       " '09050000555',\n",
       " '09050000878',\n",
       " '09050001295',\n",
       " '09050001808',\n",
       " '09050002311',\n",
       " '09050003091',\n",
       " '09050090044',\n",
       " '09050280520',\n",
       " '09053750005',\n",
       " '09056242159',\n",
       " '09057039994',\n",
       " '09058091854',\n",
       " '09058094454',\n",
       " '09058094455',\n",
       " '09058094583',\n",
       " '09058094594',\n",
       " '09058094597',\n",
       " '09058094599',\n",
       " '09058095201',\n",
       " '09058097189',\n",
       " '09058097218',\n",
       " '09058098002',\n",
       " '09058099801',\n",
       " '09061104276',\n",
       " '09061104283',\n",
       " '09061209465',\n",
       " '09061213237',\n",
       " '09061221061',\n",
       " '09061221066',\n",
       " '09061701444',\n",
       " '09061701461',\n",
       " '09061701851',\n",
       " '09061701939',\n",
       " '09061702893',\n",
       " '09061743386',\n",
       " '09061743806',\n",
       " '09061743810',\n",
       " '09061744553',\n",
       " '09061749602',\n",
       " '09061790121',\n",
       " '09061790126',\n",
       " '09063440451',\n",
       " '09063442151',\n",
       " '09063458130',\n",
       " '09064011000',\n",
       " '09064012103',\n",
       " '09064012160',\n",
       " '09064015307',\n",
       " '09064017295',\n",
       " '09064017305',\n",
       " '09064018838',\n",
       " '09064019014',\n",
       " '09064019788',\n",
       " '09065069154',\n",
       " '09065171142',\n",
       " '09065174042',\n",
       " '09065394514',\n",
       " '09065989182',\n",
       " '09066358152',\n",
       " '09066358361',\n",
       " '09066361921',\n",
       " '09066362206',\n",
       " '09066362220',\n",
       " '09066362231',\n",
       " '09066364311',\n",
       " '09066364589',\n",
       " '09066368327',\n",
       " '09066368470',\n",
       " '09066368753',\n",
       " '09066380611',\n",
       " '09066382422',\n",
       " '09066612661',\n",
       " '09066649731from',\n",
       " '09066660100',\n",
       " '09071512432',\n",
       " '09071517866',\n",
       " '09077818151',\n",
       " '09094100151',\n",
       " '09094646631',\n",
       " '09094646899',\n",
       " '09096102316',\n",
       " '09099726395',\n",
       " '09099726553',\n",
       " '09111030116',\n",
       " '09111032124',\n",
       " '09701213186',\n",
       " '0a',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000call',\n",
       " '1000s',\n",
       " '100p',\n",
       " '100percent',\n",
       " '100txt',\n",
       " '1013',\n",
       " '1030',\n",
       " '10am',\n",
       " '10k',\n",
       " '10p',\n",
       " '10ppm',\n",
       " '10th',\n",
       " '11',\n",
       " '113',\n",
       " '114',\n",
       " '116',\n",
       " '118p',\n",
       " '11mths',\n",
       " '11pm',\n",
       " '12',\n",
       " '121',\n",
       " '1225',\n",
       " '123',\n",
       " '125',\n",
       " '125gift',\n",
       " '128',\n",
       " '12hours',\n",
       " '12hrs',\n",
       " '12mths',\n",
       " '13',\n",
       " '130',\n",
       " '1327',\n",
       " '14',\n",
       " '1405',\n",
       " '140ppm',\n",
       " '145',\n",
       " '1450',\n",
       " '146tf150p',\n",
       " '14thmarch',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '150p',\n",
       " '150p16',\n",
       " '150pm',\n",
       " '150ppermesssubscription',\n",
       " '150ppm',\n",
       " '150ppmpobox10183bhamb64xe',\n",
       " '150pw',\n",
       " '151',\n",
       " '153',\n",
       " '15541',\n",
       " '15pm',\n",
       " '16',\n",
       " '1680',\n",
       " '169',\n",
       " '177',\n",
       " '18',\n",
       " '1843',\n",
       " '18p',\n",
       " '18yrs',\n",
       " '195',\n",
       " '1956669',\n",
       " '1apple',\n",
       " '1b6a5ecef91ff9',\n",
       " '1cup',\n",
       " '1er',\n",
       " '1hr',\n",
       " '1lemon',\n",
       " '1mega',\n",
       " '1million',\n",
       " '1pm',\n",
       " '1st',\n",
       " '1st4terms',\n",
       " '1stchoice',\n",
       " '1stone',\n",
       " '1thing',\n",
       " '1tulsi',\n",
       " '1win150ppmx3',\n",
       " '1winaweek',\n",
       " '1winawk',\n",
       " '1x150p',\n",
       " '1yf',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '200p',\n",
       " '2025050',\n",
       " '20m12aq',\n",
       " '20p',\n",
       " '21',\n",
       " '21870000',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '220cm2',\n",
       " '2309',\n",
       " '23f',\n",
       " '23g',\n",
       " '24',\n",
       " '24hrs',\n",
       " '24m',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '250k',\n",
       " '255',\n",
       " '25p',\n",
       " '26',\n",
       " '2667',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '2814032',\n",
       " '28days',\n",
       " '28th',\n",
       " '29',\n",
       " '2b',\n",
       " '2bold',\n",
       " '2c',\n",
       " '2channel',\n",
       " '2day',\n",
       " '2end',\n",
       " '2exit',\n",
       " '2find',\n",
       " '2getha',\n",
       " '2geva',\n",
       " '2go',\n",
       " '2hrs',\n",
       " '2kbsubject',\n",
       " '2lands',\n",
       " '2marrow',\n",
       " '2moro',\n",
       " '2morow',\n",
       " '2morro',\n",
       " '2morrow',\n",
       " '2morrowxxxx',\n",
       " '2mrw',\n",
       " '2nd',\n",
       " '2nite',\n",
       " '2optout',\n",
       " '2p',\n",
       " '2price',\n",
       " '2px',\n",
       " '2rcv',\n",
       " '2stop',\n",
       " '2stoptx',\n",
       " '2stoptxt',\n",
       " '2u',\n",
       " '2u2',\n",
       " '2waxsto',\n",
       " '2wks',\n",
       " '2wt',\n",
       " '2years',\n",
       " '2yr',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '300603',\n",
       " '300603t',\n",
       " '3030',\n",
       " '30ish',\n",
       " '30pm',\n",
       " '30pp',\n",
       " '30s',\n",
       " '30th',\n",
       " '31',\n",
       " '3100',\n",
       " '310303',\n",
       " '31p',\n",
       " '32',\n",
       " '326',\n",
       " '33',\n",
       " '350',\n",
       " '3510i',\n",
       " '3650',\n",
       " '36504',\n",
       " '3680',\n",
       " '373',\n",
       " '3750',\n",
       " '37819',\n",
       " '382',\n",
       " '391784',\n",
       " '3aj',\n",
       " '3d',\n",
       " '3g',\n",
       " '3gbp',\n",
       " '3hrs',\n",
       " '3lp',\n",
       " '3miles',\n",
       " '3mins',\n",
       " '3mobile',\n",
       " '3optical',\n",
       " '3pound',\n",
       " '3qxj9',\n",
       " '3rd',\n",
       " '3ss',\n",
       " '3uz',\n",
       " '3wks',\n",
       " '3xx',\n",
       " '3xå',\n",
       " '40',\n",
       " '400',\n",
       " '400mins',\n",
       " '402',\n",
       " '4041',\n",
       " '40411',\n",
       " '40533',\n",
       " '40gb',\n",
       " '4217',\n",
       " '42478',\n",
       " '430',\n",
       " '434',\n",
       " '44',\n",
       " '440',\n",
       " '4403ldnw1a7rw18',\n",
       " '447801259231',\n",
       " '448712404000',\n",
       " '449050000301',\n",
       " '45',\n",
       " '450',\n",
       " '450p',\n",
       " '45239',\n",
       " '45pm',\n",
       " '4742',\n",
       " '48',\n",
       " '4882',\n",
       " '48922',\n",
       " '49557',\n",
       " '4a',\n",
       " '4d',\n",
       " '4eva',\n",
       " '4fil',\n",
       " '4get',\n",
       " '4goten',\n",
       " '4info',\n",
       " '4jx',\n",
       " '4mths',\n",
       " '4qf2',\n",
       " '4t',\n",
       " '4th',\n",
       " '4the',\n",
       " '4thnov',\n",
       " '4txt',\n",
       " '4u',\n",
       " '4utxt',\n",
       " '4w',\n",
       " '4wrd',\n",
       " '4years',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '505060',\n",
       " '50award',\n",
       " '50ea',\n",
       " '50gbp',\n",
       " '50p',\n",
       " '50perweeksub',\n",
       " '50perwksub',\n",
       " '50pm',\n",
       " '50pmmorefrommobile2bremoved',\n",
       " '50ppm',\n",
       " '50s',\n",
       " '515',\n",
       " '526',\n",
       " '528',\n",
       " '530',\n",
       " '54',\n",
       " '542',\n",
       " '5digital',\n",
       " '5free',\n",
       " '5k',\n",
       " '5min',\n",
       " '5p',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5wb',\n",
       " '5we',\n",
       " '5wkg',\n",
       " '5wq',\n",
       " '5years',\n",
       " '600',\n",
       " '6031',\n",
       " '60p',\n",
       " '61',\n",
       " '61200',\n",
       " '61610',\n",
       " '62220cncl',\n",
       " '62468',\n",
       " '62735',\n",
       " '630',\n",
       " '645',\n",
       " '65',\n",
       " '650',\n",
       " '66',\n",
       " '6669',\n",
       " '674',\n",
       " '67441233',\n",
       " '68866',\n",
       " '69101',\n",
       " '69200',\n",
       " '69669',\n",
       " '69696',\n",
       " '69698',\n",
       " '69855',\n",
       " '69866',\n",
       " '69888',\n",
       " '69888nyt',\n",
       " '69911',\n",
       " '69969',\n",
       " '6days',\n",
       " '6hl',\n",
       " '6hrs',\n",
       " '6ish',\n",
       " '6missed',\n",
       " '6months',\n",
       " '6ph',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6wu',\n",
       " '6zf',\n",
       " '700',\n",
       " '71',\n",
       " '7250',\n",
       " '7250i',\n",
       " '731',\n",
       " '74355',\n",
       " '75',\n",
       " '750',\n",
       " '7548',\n",
       " '75max',\n",
       " '762',\n",
       " '77',\n",
       " '7732584351',\n",
       " '78',\n",
       " '786',\n",
       " '7876150ppm',\n",
       " '79',\n",
       " '7ish',\n",
       " '7oz',\n",
       " '7pm',\n",
       " '7th',\n",
       " '7ws',\n",
       " '800',\n",
       " '8000930705',\n",
       " '80062',\n",
       " '8007',\n",
       " '80082',\n",
       " '80086',\n",
       " '80122300p',\n",
       " '80160',\n",
       " '80182',\n",
       " '8027',\n",
       " '80488',\n",
       " '80608',\n",
       " '8077',\n",
       " '80878',\n",
       " '81010',\n",
       " '81151',\n",
       " '81303',\n",
       " '81618',\n",
       " '82050',\n",
       " '82242',\n",
       " '82277',\n",
       " '82324',\n",
       " '83021',\n",
       " '83039',\n",
       " '83049',\n",
       " '83110',\n",
       " '83118',\n",
       " '83222',\n",
       " '83332',\n",
       " '83355',\n",
       " '83370',\n",
       " '83383',\n",
       " '83435',\n",
       " '83600',\n",
       " '83738',\n",
       " '84025',\n",
       " '84128',\n",
       " '84199',\n",
       " '85',\n",
       " '850',\n",
       " '85023',\n",
       " '8552',\n",
       " '85555',\n",
       " '86021',\n",
       " '861',\n",
       " '86688',\n",
       " '86888',\n",
       " '87021',\n",
       " '87066',\n",
       " '87070',\n",
       " '87077',\n",
       " '87121',\n",
       " '87131',\n",
       " '8714714',\n",
       " '872',\n",
       " '87239',\n",
       " '87575',\n",
       " '8800',\n",
       " '88039',\n",
       " '88066',\n",
       " '88088',\n",
       " '88222',\n",
       " '88600',\n",
       " '88800',\n",
       " '8883',\n",
       " '88888',\n",
       " '89034',\n",
       " '89080',\n",
       " '89123',\n",
       " '89545',\n",
       " '89555',\n",
       " '89693',\n",
       " '89938',\n",
       " '8am',\n",
       " '8lb',\n",
       " '8p',\n",
       " '8pm',\n",
       " '8th',\n",
       " '8wp',\n",
       " '900',\n",
       " '9061100010',\n",
       " '910',\n",
       " '9153',\n",
       " '9280114',\n",
       " '930',\n",
       " '946',\n",
       " '95',\n",
       " '9755',\n",
       " '9758',\n",
       " '97n7qp',\n",
       " '98321561',\n",
       " '99',\n",
       " '9996',\n",
       " '9ae',\n",
       " '9am',\n",
       " '9ja',\n",
       " '9pm',\n",
       " '9t',\n",
       " '9yt',\n",
       " '____',\n",
       " 'a21',\n",
       " 'a30',\n",
       " 'aa',\n",
       " 'aah',\n",
       " 'aaniye',\n",
       " 'aaooooright',\n",
       " 'aathi',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abdomen',\n",
       " 'abel',\n",
       " 'abi',\n",
       " 'ability',\n",
       " 'abiola',\n",
       " 'abj',\n",
       " 'able',\n",
       " 'abnormally',\n",
       " 'about',\n",
       " 'aboutas',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absolutly',\n",
       " 'abstract',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'aburo',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accidant',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accommodationvouchers',\n",
       " 'accomodate',\n",
       " 'accomodations',\n",
       " 'accordin',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accumulation',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'acid',\n",
       " 'acl03530150pm',\n",
       " 'acnt',\n",
       " 'aco',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activ8',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activities',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addamsfa',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addie',\n",
       " 'adding',\n",
       " 'address',\n",
       " 'adds',\n",
       " 'adewale',\n",
       " 'admin',\n",
       " 'administrator',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'adore',\n",
       " 'adress',\n",
       " 'adrian',\n",
       " 'ads',\n",
       " 'adsense',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'adventure',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advisors',\n",
       " 'aeronautics',\n",
       " 'aeroplane',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affections',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftr',\n",
       " 'ag',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agalla',\n",
       " 'age',\n",
       " 'age16',\n",
       " 'age23',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agidhane',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahhhh',\n",
       " 'ahmad',\n",
       " 'ahold',\n",
       " 'aids',\n",
       " 'aig',\n",
       " 'aight',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'air1',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyah',\n",
       " 'aiyar',\n",
       " 'aiyo',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akon',\n",
       " 'al',\n",
       " 'alaikkum',\n",
       " 'alaipayuthe',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'aldrine',\n",
       " 'alert',\n",
       " 'alertfrom',\n",
       " 'alerts',\n",
       " 'aletter',\n",
       " 'alex',\n",
       " 'alfie',\n",
       " 'algarve',\n",
       " 'algebra',\n",
       " 'ali',\n",
       " 'alian',\n",
       " 'alibi',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allalo',\n",
       " 'allday',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'alrite',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'aluable',\n",
       " 'alwa',\n",
       " 'always',\n",
       " 'alwys',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'ambitious',\n",
       " 'ambrith',\n",
       " 'american',\n",
       " 'amigos',\n",
       " 'ammae',\n",
       " 'ammo',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amplikater',\n",
       " 'ams',\n",
       " 'amt',\n",
       " 'amused',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'analysis',\n",
       " 'anand',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andres',\n",
       " 'andrews',\n",
       " 'andros',\n",
       " 'angry',\n",
       " 'animation',\n",
       " 'anjie',\n",
       " 'anjola',\n",
       " 'anna',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'announcement',\n",
       " 'annoying',\n",
       " 'anot',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerin',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'answr',\n",
       " 'antelope',\n",
       " 'antibiotic',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyones',\n",
       " 'anyplaces',\n",
       " 'anythiing',\n",
       " 'anythin',\n",
       " 'anything',\n",
       " 'anythingtomorrow',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apeshit',\n",
       " 'aphexåõs',\n",
       " 'apo',\n",
       " 'apologise',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'applausestore',\n",
       " 'applebees',\n",
       " 'apples',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'applyed',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'appy',\n",
       " 'april',\n",
       " 'apt',\n",
       " 'aptitude',\n",
       " 'aquarius',\n",
       " 'ar',\n",
       " 'arabian',\n",
       " 'arcade',\n",
       " 'archive',\n",
       " 'ard',\n",
       " 'are',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arestaurant',\n",
       " 'aretaking',\n",
       " 'areyouunique',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'aries',\n",
       " 'arise',\n",
       " 'arises',\n",
       " 'arithmetic',\n",
       " 'arm',\n",
       " 'armand',\n",
       " 'armenia',\n",
       " 'arms',\n",
       " 'arng',\n",
       " 'arngd',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.237314</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.361385</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.228624</td>\n",
       "      <td>000pes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.260735</td>\n",
       "      <td>008704050406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.242052</td>\n",
       "      <td>0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>7349</td>\n",
       "      <td>0.566275</td>\n",
       "      <td>û_thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>7350</td>\n",
       "      <td>0.284009</td>\n",
       "      <td>ûï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>7351</td>\n",
       "      <td>0.278142</td>\n",
       "      <td>ûïharry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7352</th>\n",
       "      <td>7352</td>\n",
       "      <td>0.362962</td>\n",
       "      <td>ûò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7353</th>\n",
       "      <td>7353</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>ûówell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7354 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     tfidf          word\n",
       "0         0  0.237314            00\n",
       "1         1  0.361385           000\n",
       "2         2  0.228624        000pes\n",
       "3         3  0.260735  008704050406\n",
       "4         4  0.242052          0089\n",
       "...     ...       ...           ...\n",
       "7349   7349  0.566275      û_thanks\n",
       "7350   7350  0.284009            ûï\n",
       "7351   7351  0.278142       ûïharry\n",
       "7352   7352  0.362962            ûò\n",
       "7353   7353  0.348800        ûówell\n",
       "\n",
       "[7354 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max = nonzero_tfidf_df.groupby('index').max()\n",
    "tfidf_max = tfidf_max.reset_index()\n",
    "tfidf_max['word'] = [vect_tfidf.get_feature_names()[idx] for idx in tfidf_max['index']]\n",
    "tfidf_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tfidf_max.sort_values(['tfidf','word'],ascending=[False,True]).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>146tf150p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>anytime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>1214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>beerage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>2298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>er</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>3180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>3293</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>3848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>lei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>4523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>nite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>4646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4651</th>\n",
       "      <td>4651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>okie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6434</th>\n",
       "      <td>6434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6439</th>\n",
       "      <td>6439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>thanx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6598</th>\n",
       "      <td>6598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>7089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7328</th>\n",
       "      <td>7328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>yup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517</th>\n",
       "      <td>6517</td>\n",
       "      <td>0.980166</td>\n",
       "      <td>tick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1300</td>\n",
       "      <td>0.932702</td>\n",
       "      <td>blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     tfidf       word\n",
       "274     274  1.000000  146tf150p\n",
       "533     533  1.000000        645\n",
       "931     931  1.000000   anything\n",
       "933     933  1.000000    anytime\n",
       "1214   1214  1.000000    beerage\n",
       "2298   2298  1.000000       done\n",
       "2496   2496  1.000000         er\n",
       "3180   3180  1.000000     havent\n",
       "3293   3293  1.000000       home\n",
       "3848   3848  1.000000        lei\n",
       "4523   4523  1.000000       nite\n",
       "4646   4646  1.000000         ok\n",
       "4651   4651  1.000000       okie\n",
       "6434   6434  1.000000      thank\n",
       "6439   6439  1.000000      thanx\n",
       "6598   6598  1.000000        too\n",
       "7089   7089  1.000000      where\n",
       "7328   7328  1.000000        yup\n",
       "6517   6517  0.980166       tick\n",
       "1300   1300  0.932702      blank"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "146tf150p    1.000000\n",
       "645          1.000000\n",
       "anything     1.000000\n",
       "anytime      1.000000\n",
       "beerage      1.000000\n",
       "done         1.000000\n",
       "er           1.000000\n",
       "havent       1.000000\n",
       "home         1.000000\n",
       "lei          1.000000\n",
       "nite         1.000000\n",
       "ok           1.000000\n",
       "okie         1.000000\n",
       "thank        1.000000\n",
       "thanx        1.000000\n",
       "too          1.000000\n",
       "where        1.000000\n",
       "yup          1.000000\n",
       "tick         0.980166\n",
       "blank        0.932702\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list(m['tfidf']),index=m['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.184719</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.193701</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.228624</td>\n",
       "      <td>000pes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.251494</td>\n",
       "      <td>008704050406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.242052</td>\n",
       "      <td>0089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>7349</td>\n",
       "      <td>0.566275</td>\n",
       "      <td>û_thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>7350</td>\n",
       "      <td>0.284009</td>\n",
       "      <td>ûï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>7351</td>\n",
       "      <td>0.278142</td>\n",
       "      <td>ûïharry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7352</th>\n",
       "      <td>7352</td>\n",
       "      <td>0.270956</td>\n",
       "      <td>ûò</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7353</th>\n",
       "      <td>7353</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>ûówell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7354 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     tfidf          word\n",
       "0         0  0.184719            00\n",
       "1         1  0.193701           000\n",
       "2         2  0.228624        000pes\n",
       "3         3  0.251494  008704050406\n",
       "4         4  0.242052          0089\n",
       "...     ...       ...           ...\n",
       "7349   7349  0.566275      û_thanks\n",
       "7350   7350  0.284009            ûï\n",
       "7351   7351  0.278142       ûïharry\n",
       "7352   7352  0.270956            ûò\n",
       "7353   7353  0.348800        ûówell\n",
       "\n",
       "[7354 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_min = nonzero_tfidf_df.groupby('index').min()\n",
    "tfidf_min = tfidf_min.reset_index()\n",
    "tfidf_min['word'] = [vect_tfidf.get_feature_names()[idx] for idx in tfidf_max['index']]\n",
    "tfidf_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>6561</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>3558</td>\n",
       "      <td>0.027758</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6446</th>\n",
       "      <td>6446</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>4564</td>\n",
       "      <td>0.031338</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>3548</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>1044</td>\n",
       "      <td>0.031934</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>1193</td>\n",
       "      <td>0.032058</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>3445</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>3336</td>\n",
       "      <td>0.034464</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>849</td>\n",
       "      <td>0.034802</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7310</th>\n",
       "      <td>7310</td>\n",
       "      <td>0.034835</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>4163</td>\n",
       "      <td>0.035388</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>4673</td>\n",
       "      <td>0.036832</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>5944</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>4622</td>\n",
       "      <td>0.037753</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>7305</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4466</th>\n",
       "      <td>4466</td>\n",
       "      <td>0.038990</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>896</td>\n",
       "      <td>0.040236</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>1541</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>7149</td>\n",
       "      <td>0.043093</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     tfidf  word\n",
       "6561   6561  0.023205    to\n",
       "3558   3558  0.027758    it\n",
       "6446   6446  0.028577   the\n",
       "4564   4564  0.031338   not\n",
       "3548   3548  0.031698    is\n",
       "1044   1044  0.031934    at\n",
       "1193   1193  0.032058    be\n",
       "3445   3445  0.032877    in\n",
       "3336   3336  0.034464   how\n",
       "849     849  0.034802   all\n",
       "7310   7310  0.034835  your\n",
       "4163   4163  0.035388    me\n",
       "4673   4673  0.036832  only\n",
       "5944   5944  0.037647    so\n",
       "4622   4622  0.037753    of\n",
       "7305   7305  0.038438   you\n",
       "4466   4466  0.038990  need\n",
       "896     896  0.040236   and\n",
       "1541   1541  0.040318   can\n",
       "7149   7149  0.043093  with"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_min.sort_values(['tfidf','word'],ascending=[True,True]).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def answer_four():\n",
    "    vect_tfidf = TfidfVectorizer().fit(X_train)\n",
    "    X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "    \n",
    "    nonzero = X_train_vectorized.nonzero()\n",
    "    nonzero_tfidf = [X_train_vectorized[nonzero[0][idx],nonzero[1][idx]] for idx in range(len(nonzero[0]))]\n",
    "    nonzero_tfidf_df = pd.DataFrame(nonzero_tfidf,index=nonzero[1],columns=['tfidf'])\n",
    "    nonzero_tfidf_df = nonzero_tfidf_df.reset_index()\n",
    "    \n",
    "    tfidf_max = nonzero_tfidf_df.groupby('index').max()\n",
    "    tfidf_max = tfidf_max.reset_index()\n",
    "    tfidf_max['word'] = [vect_tfidf.get_feature_names()[idx] for idx in tfidf_max['index']]\n",
    "    tfidf_max = tfidf_max.sort_values(['tfidf','word'],ascending=[False,True]).iloc[:20]\n",
    "    \n",
    "    tfidf_min = nonzero_tfidf_df.groupby('index').max()\n",
    "    tfidf_min = tfidf_min.reset_index()\n",
    "    tfidf_min['word'] = [vect_tfidf.get_feature_names()[idx] for idx in tfidf_min['index']]\n",
    "    tfidf_min = tfidf_min.sort_values(['tfidf','word'],ascending=[True,True]).iloc[:20]\n",
    "    \n",
    "    result_max = pd.Series(list(tfidf_max['tfidf']),index=tfidf_max['word'])\n",
    "    result_min = pd.Series(list(tfidf_min['tfidf']),index=tfidf_min['word'])\n",
    "    \n",
    "    return (result_min,result_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(word\n",
       " aaniye          0.074475\n",
       " athletic        0.074475\n",
       " chef            0.074475\n",
       " companion       0.074475\n",
       " courageous      0.074475\n",
       " dependable      0.074475\n",
       " determined      0.074475\n",
       " exterminator    0.074475\n",
       " healer          0.074475\n",
       " listener        0.074475\n",
       " organizer       0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " psychologist    0.074475\n",
       " pudunga         0.074475\n",
       " stylist         0.074475\n",
       " sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " afternoons      0.091250\n",
       " approaching     0.091250\n",
       " dtype: float64,\n",
       " word\n",
       " 146tf150p    1.000000\n",
       " 645          1.000000\n",
       " anything     1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " done         1.000000\n",
       " er           1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " ok           1.000000\n",
       " okie         1.000000\n",
       " thank        1.000000\n",
       " thanx        1.000000\n",
       " too          1.000000\n",
       " where        1.000000\n",
       " yup          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ll': 1164,\n",
       " 'text': 1947,\n",
       " 'you': 2281,\n",
       " 'when': 2191,\n",
       " 'drop': 656,\n",
       " 'off': 1400,\n",
       " 'hi': 940,\n",
       " 'mate': 1236,\n",
       " 'its': 1046,\n",
       " 'did': 606,\n",
       " 'hav': 918,\n",
       " 'nice': 1364,\n",
       " 'just': 1077,\n",
       " 'message': 1264,\n",
       " 'say': 1685,\n",
       " 'hello': 934,\n",
       " 'coz': 539,\n",
       " 'sent': 1718,\n",
       " 'in': 1006,\n",
       " 'ages': 172,\n",
       " 'started': 1850,\n",
       " 'driving': 655,\n",
       " 'so': 1800,\n",
       " 'stay': 1855,\n",
       " 'network': 1355,\n",
       " 'operator': 1425,\n",
       " 'the': 1958,\n",
       " 'service': 1723,\n",
       " 'is': 1037,\n",
       " 'free': 800,\n",
       " 'for': 790,\n",
       " 'visit': 2121,\n",
       " '80488': 114,\n",
       " 'biz': 332,\n",
       " 'activate': 152,\n",
       " 'your': 2282,\n",
       " '500': 95,\n",
       " 'messages': 1265,\n",
       " 'by': 393,\n",
       " 'replying': 1630,\n",
       " 'to': 2003,\n",
       " 'this': 1973,\n",
       " 'with': 2218,\n",
       " 'word': 2234,\n",
       " 'terms': 1944,\n",
       " 'conditions': 509,\n",
       " 'www': 2253,\n",
       " 'com': 492,\n",
       " 'girls': 853,\n",
       " 'are': 233,\n",
       " 'waiting': 2141,\n",
       " 'them': 1961,\n",
       " 'now': 1385,\n",
       " 'great': 879,\n",
       " 'night': 1367,\n",
       " 'chatting': 452,\n",
       " 'send': 1715,\n",
       " 'stop': 1863,\n",
       " 'how': 973,\n",
       " 'much': 1330,\n",
       " 'an': 205,\n",
       " 'can': 414,\n",
       " 'further': 825,\n",
       " 'club': 481,\n",
       " 'tones': 2014,\n",
       " 'see': 1703,\n",
       " 'my': 1338,\n",
       " 'tone': 2013,\n",
       " 'enjoy': 693,\n",
       " 'cost': 529,\n",
       " '50': 94,\n",
       " 'week': 2174,\n",
       " 'good': 869,\n",
       " 'morning': 1312,\n",
       " 'princess': 1551,\n",
       " 'since': 1767,\n",
       " 'we': 2165,\n",
       " 'out': 1442,\n",
       " 'already': 195,\n",
       " 'part': 1458,\n",
       " 'of': 1399,\n",
       " 'checking': 458,\n",
       " 'iq': 1036,\n",
       " 'at': 256,\n",
       " 'home': 958,\n",
       " 'please': 1506,\n",
       " 'call': 400,\n",
       " 'dnt': 628,\n",
       " 'wnt': 2225,\n",
       " 'wid': 2203,\n",
       " 'today': 2006,\n",
       " 'offer': 1401,\n",
       " 'claim': 474,\n",
       " 'ur': 2087,\n",
       " '150': 44,\n",
       " 'worth': 2245,\n",
       " 'discount': 623,\n",
       " 'vouchers': 2130,\n",
       " 'yes': 2272,\n",
       " '85023': 122,\n",
       " 'savamob': 1682,\n",
       " 'member': 1259,\n",
       " 'offers': 1402,\n",
       " 'mobile': 1292,\n",
       " 'cs': 553,\n",
       " '00': 0,\n",
       " 'sub': 1881,\n",
       " '16': 48,\n",
       " 'unsub': 2080,\n",
       " 'reply': 1629,\n",
       " 'do': 629,\n",
       " 'knw': 1100,\n",
       " 'dis': 622,\n",
       " 'no': 1370,\n",
       " 'lt': 1201,\n",
       " 'gt': 884,\n",
       " 'cool': 525,\n",
       " 'few': 759,\n",
       " 'said': 1675,\n",
       " 'kiss': 1095,\n",
       " 'sound': 1826,\n",
       " 'he': 924,\n",
       " 'gorgeous': 873,\n",
       " 'man': 1223,\n",
       " 'isn': 1041,\n",
       " 'kind': 1092,\n",
       " 'person': 1473,\n",
       " 'who': 2199,\n",
       " 'needs': 1352,\n",
       " 'smile': 1791,\n",
       " 'his': 943,\n",
       " 'day': 579,\n",
       " 'guy': 890,\n",
       " 'some': 1805,\n",
       " 'but': 386,\n",
       " 'like': 1151,\n",
       " 'be': 297,\n",
       " 'interested': 1026,\n",
       " 'buying': 389,\n",
       " 'something': 1810,\n",
       " 'else': 682,\n",
       " 'next': 1362,\n",
       " 'and': 206,\n",
       " 'gave': 834,\n",
       " 'it': 1044,\n",
       " 'us': 2094,\n",
       " 've': 2110,\n",
       " 'reached': 1594,\n",
       " 'sch': 1690,\n",
       " 'him': 942,\n",
       " 'ok': 1407,\n",
       " 'afternoon': 166,\n",
       " 'love': 1192,\n",
       " 'was': 2155,\n",
       " 'words': 2235,\n",
       " 'on': 1414,\n",
       " 'ym': 2277,\n",
       " 'get': 846,\n",
       " 'tm': 2000,\n",
       " 'very': 2111,\n",
       " 'move': 1316,\n",
       " 'slave': 1784,\n",
       " 'smiles': 1792,\n",
       " 'drink': 651,\n",
       " 'coffee': 485,\n",
       " 'await': 267,\n",
       " 'sms': 1797,\n",
       " 'auction': 262,\n",
       " 'have': 919,\n",
       " 'won': 2229,\n",
       " 'nokia': 1374,\n",
       " '7250i': 106,\n",
       " 'what': 2188,\n",
       " 'win': 2209,\n",
       " 'our': 1441,\n",
       " 'take': 1915,\n",
       " '86021': 123,\n",
       " 'hg': 939,\n",
       " 'suite342': 1889,\n",
       " '2lands': 68,\n",
       " 'row': 1662,\n",
       " 'w1jhl': 2135,\n",
       " 'cant': 418,\n",
       " 'talk': 1918,\n",
       " 'will': 2207,\n",
       " 'dont': 642,\n",
       " 'keep': 1084,\n",
       " 'calling': 406,\n",
       " '1st': 51,\n",
       " 'wk': 2222,\n",
       " 'gr8': 877,\n",
       " 'each': 667,\n",
       " 'txt': 2054,\n",
       " '8007': 113,\n",
       " 'or': 1430,\n",
       " 'hit': 945,\n",
       " 'polys': 1527,\n",
       " '150p': 45,\n",
       " 'poly': 1524,\n",
       " 'lets': 1142,\n",
       " 'make': 1220,\n",
       " 'saturday': 1681,\n",
       " 'monday': 1303,\n",
       " 'as': 246,\n",
       " 'per': 1471,\n",
       " 'then': 1962,\n",
       " 'why': 2202,\n",
       " 'came': 411,\n",
       " 'hostel': 967,\n",
       " 'god': 862,\n",
       " 'picked': 1483,\n",
       " 'up': 2083,\n",
       " 'flower': 782,\n",
       " 'touched': 2024,\n",
       " 'friend': 808,\n",
       " '4u': 93,\n",
       " 'dint': 618,\n",
       " 'slept': 1787,\n",
       " 'wah': 2138,\n",
       " 'lucky': 1204,\n",
       " 'save': 1683,\n",
       " 'money': 1304,\n",
       " 'hee': 930,\n",
       " 'oh': 1405,\n",
       " 'ho': 951,\n",
       " 'first': 772,\n",
       " 'time': 1993,\n",
       " 'use': 2095,\n",
       " 'these': 1965,\n",
       " 'type': 2060,\n",
       " 'unlimited': 2077,\n",
       " 'texts': 1948,\n",
       " 'minutes': 1279,\n",
       " 'things': 1968,\n",
       " 'quick': 1573,\n",
       " 'question': 1571,\n",
       " 'thats': 1956,\n",
       " 'cover': 538,\n",
       " 'face': 733,\n",
       " 'hot': 968,\n",
       " 'cum': 555,\n",
       " 'surely': 1902,\n",
       " 'result': 1638,\n",
       " 'line': 1154,\n",
       " 'hurt': 985,\n",
       " 'me': 1244,\n",
       " 'truth': 2041,\n",
       " 'sorry': 1821,\n",
       " 'later': 1114,\n",
       " 'meeting': 1256,\n",
       " 'happening': 910,\n",
       " 'job': 1062,\n",
       " 'game': 830,\n",
       " 'phone': 1477,\n",
       " 'lol': 1173,\n",
       " 'really': 1602,\n",
       " 'into': 1030,\n",
       " 'fact': 735,\n",
       " 'that': 1955,\n",
       " 'gets': 847,\n",
       " 'shit': 1741,\n",
       " 'all': 191,\n",
       " 'over': 1446,\n",
       " 'arms': 240,\n",
       " 'every': 713,\n",
       " 'five': 773,\n",
       " 'asked': 250,\n",
       " 'pick': 1482,\n",
       " 'right': 1646,\n",
       " 'pls': 1510,\n",
       " 'dunno': 664,\n",
       " 'lei': 1136,\n",
       " 'neva': 1358,\n",
       " 'roger': 1653,\n",
       " 'from': 815,\n",
       " 'come': 494,\n",
       " 'tomorrow': 2012,\n",
       " 'tell': 1936,\n",
       " 'her': 936,\n",
       " 'eat': 673,\n",
       " 'hey': 938,\n",
       " 'pple': 1539,\n",
       " '900': 132,\n",
       " 'nights': 1368,\n",
       " 'excellent': 722,\n",
       " 'wif': 2204,\n",
       " 'she': 1737,\n",
       " 'mind': 1273,\n",
       " 'if': 996,\n",
       " 'go': 861,\n",
       " 'bedroom': 303,\n",
       " 'minute': 1278,\n",
       " 'sed': 1702,\n",
       " 'sexy': 1731,\n",
       " 'mood': 1308,\n",
       " 'minuts': 1280,\n",
       " 'latr': 1116,\n",
       " 'cake': 397,\n",
       " 'wife': 2205,\n",
       " 'accidentally': 147,\n",
       " 'deleted': 596,\n",
       " 'mm': 1286,\n",
       " 'am': 200,\n",
       " 'way': 2163,\n",
       " 'nvm': 1392,\n",
       " 'there': 1964,\n",
       " 'seems': 1707,\n",
       " 'put': 1567,\n",
       " 'skip': 1781,\n",
       " 'outside': 1443,\n",
       " 'house': 972,\n",
       " 'which': 2196,\n",
       " 'before': 306,\n",
       " 'video': 2114,\n",
       " 'download': 646,\n",
       " 'google': 872,\n",
       " 'search': 1696,\n",
       " 'coming': 497,\n",
       " 'guaranteed': 885,\n",
       " 'latest': 1115,\n",
       " '40gb': 87,\n",
       " 'ipod': 1035,\n",
       " 'mp3': 1319,\n",
       " 'player': 1503,\n",
       " 'prize': 1553,\n",
       " 'collect': 488,\n",
       " '83355': 119,\n",
       " 'mtmsgrcvd18': 1328,\n",
       " 'got': 874,\n",
       " 'babe': 275,\n",
       " 'still': 1859,\n",
       " 'awake': 269,\n",
       " 'college': 490,\n",
       " 'current': 557,\n",
       " 'situation': 1777,\n",
       " 'ask': 248,\n",
       " 'should': 1750,\n",
       " 'someone': 1808,\n",
       " 'check': 456,\n",
       " 'same': 1679,\n",
       " 'thing': 1967,\n",
       " 'medical': 1252,\n",
       " 'nigeria': 1366,\n",
       " 'less': 1138,\n",
       " 'unless': 2076,\n",
       " 'getting': 849,\n",
       " 'rates': 1587,\n",
       " 'new': 1360,\n",
       " 'content': 520,\n",
       " 'weekend': 2175,\n",
       " 'been': 304,\n",
       " 'missing': 1284,\n",
       " 'sp': 1829,\n",
       " 'space': 1830,\n",
       " 'sir': 1771,\n",
       " 'well': 2182,\n",
       " 'until': 2082,\n",
       " 'class': 475,\n",
       " 'pm': 1513,\n",
       " 'hope': 962,\n",
       " 'late': 1113,\n",
       " 'place': 1494,\n",
       " 'always': 199,\n",
       " 'anything': 220,\n",
       " 'about': 140,\n",
       " 're': 1592,\n",
       " 'done': 641,\n",
       " 'billed': 324,\n",
       " 'netcollex': 1354,\n",
       " 'ltd': 1202,\n",
       " 'po': 1514,\n",
       " 'box': 357,\n",
       " 'eatin': 674,\n",
       " 'lunch': 1205,\n",
       " 'weather': 2169,\n",
       " 'only': 1420,\n",
       " 'not': 1381,\n",
       " 'shopping': 1745,\n",
       " 'official': 1404,\n",
       " 'england': 692,\n",
       " 'ringtone': 1648,\n",
       " 'colour': 491,\n",
       " 'flag': 776,\n",
       " 'yer': 2271,\n",
       " '84199': 121,\n",
       " 'optout': 1429,\n",
       " 'eng': 691,\n",
       " 'box39822': 358,\n",
       " 'w111wx': 2133,\n",
       " 'top': 2019,\n",
       " 'weekly': 2177,\n",
       " 'has': 915,\n",
       " 'dating': 578,\n",
       " 'entered': 697,\n",
       " 'because': 299,\n",
       " 'they': 1966,\n",
       " 'fancy': 740,\n",
       " 'find': 767,\n",
       " 'landline': 1108,\n",
       " 'registered': 1617,\n",
       " 'log': 1170,\n",
       " 'enter': 696,\n",
       " 'confirm': 511,\n",
       " 'beware': 319,\n",
       " 'share': 1735,\n",
       " 'anyone': 218,\n",
       " 'pic': 1481,\n",
       " 'prob': 1554,\n",
       " 'after': 165,\n",
       " 'forgot': 794,\n",
       " 'ì_': 2291,\n",
       " 'smth': 1798,\n",
       " 'card': 420,\n",
       " 'da': 565,\n",
       " 'present': 1544,\n",
       " 'ìï': 2292,\n",
       " 'want': 2149,\n",
       " 'write': 2250,\n",
       " 'sign': 1762,\n",
       " 'fighting': 761,\n",
       " 'world': 2240,\n",
       " 'easy': 672,\n",
       " 'either': 681,\n",
       " 'lose': 1184,\n",
       " 'bt': 375,\n",
       " 'some1': 1806,\n",
       " 'close': 478,\n",
       " 'were': 2186,\n",
       " 'working': 2238,\n",
       " 'wanna': 2148,\n",
       " 'chat': 451,\n",
       " 'bored': 348,\n",
       " 'etc': 707,\n",
       " 'ring': 1647,\n",
       " 'xx': 2258,\n",
       " 'tmr': 2001,\n",
       " 'lar': 1110,\n",
       " 'aiya': 183,\n",
       " 'mayb': 1242,\n",
       " 'set': 1725,\n",
       " 'help': 935,\n",
       " 'bishan': 329,\n",
       " 'need': 1351,\n",
       " 'buy': 388,\n",
       " 'early': 669,\n",
       " 'cos': 528,\n",
       " 'gotta': 876,\n",
       " 'park': 1456,\n",
       " 'car': 419,\n",
       " 'joined': 1065,\n",
       " 'room': 1657,\n",
       " 'number': 1389,\n",
       " 'again': 168,\n",
       " 'sure': 1901,\n",
       " 'door': 643,\n",
       " 'give': 855,\n",
       " 'walk': 2144,\n",
       " 'company': 500,\n",
       " 'food': 787,\n",
       " 'down': 645,\n",
       " 'remember': 1623,\n",
       " 'chance': 445,\n",
       " 'reality': 1600,\n",
       " 'fantasy': 743,\n",
       " 'show': 1752,\n",
       " '08707509020': 15,\n",
       " '20p': 59,\n",
       " 'min': 1272,\n",
       " 'ntt': 1387,\n",
       " '1327': 41,\n",
       " 'croydon': 551,\n",
       " 'cr9': 540,\n",
       " '5wb': 102,\n",
       " '0870': 13,\n",
       " 'national': 1344,\n",
       " 'rate': 1586,\n",
       " 'fool': 788,\n",
       " 'more': 1311,\n",
       " 'questions': 1572,\n",
       " 'than': 1950,\n",
       " 'wise': 2213,\n",
       " 'answer': 212,\n",
       " 'know': 1098,\n",
       " 'during': 665,\n",
       " 'gm': 859,\n",
       " 'thk': 1974,\n",
       " 'juz': 1078,\n",
       " 'wat': 2158,\n",
       " 'yest': 2273,\n",
       " 'lor': 1183,\n",
       " 'except': 723,\n",
       " 'kb': 1083,\n",
       " 'sun': 1892,\n",
       " 'nt': 1386,\n",
       " 'lesson': 1139,\n",
       " 'attend': 261,\n",
       " 'sat': 1680,\n",
       " 'where': 2193,\n",
       " 'around': 241,\n",
       " 'fine': 768,\n",
       " 'tonight': 2015,\n",
       " 'gentle': 841,\n",
       " 'baby': 277,\n",
       " 'soon': 1818,\n",
       " 'taking': 1917,\n",
       " 'inches': 1009,\n",
       " 'deep': 592,\n",
       " 'inside': 1021,\n",
       " 'pussy': 1566,\n",
       " 'cheap': 453,\n",
       " 'raining': 1582,\n",
       " 'mah': 1215,\n",
       " 'hard': 914,\n",
       " 'leave': 1127,\n",
       " 'orchard': 1432,\n",
       " 'shuhui': 1756,\n",
       " 'change': 446,\n",
       " 'noe': 1373,\n",
       " 'sister': 1773,\n",
       " 'going': 865,\n",
       " 'amp': 203,\n",
       " 'vodka': 2125,\n",
       " 'address': 158,\n",
       " 'urgent': 2090,\n",
       " 'complimentary': 504,\n",
       " 'tenerife': 1943,\n",
       " 'holiday': 955,\n",
       " '10': 25,\n",
       " '000': 1,\n",
       " 'cash': 430,\n",
       " 'collection': 489,\n",
       " 'sae': 1673,\n",
       " '150ppm': 47,\n",
       " '18': 49,\n",
       " 'todays': 2007,\n",
       " 'voda': 2123,\n",
       " 'numbers': 1390,\n",
       " 'ending': 687,\n",
       " 'selected': 1709,\n",
       " 'receive': 1607,\n",
       " '350': 78,\n",
       " 'award': 270,\n",
       " 'match': 1234,\n",
       " '08712300220': 16,\n",
       " 'quoting': 1579,\n",
       " 'code': 484,\n",
       " 'standard': 1848,\n",
       " 'app': 225,\n",
       " 'didnt': 608,\n",
       " 'complete': 502,\n",
       " 'alright': 196,\n",
       " 'took': 2018,\n",
       " 'back': 278,\n",
       " 'yo': 2278,\n",
       " 'switch': 1910,\n",
       " 'fone': 786,\n",
       " 'works': 2239,\n",
       " 'years': 2269,\n",
       " 'old': 1411,\n",
       " 'doesnt': 634,\n",
       " 'bother': 354,\n",
       " 'date': 576,\n",
       " '11': 33,\n",
       " '10p': 32,\n",
       " 'parents': 1454,\n",
       " 'hand': 902,\n",
       " 'tired': 1996,\n",
       " 'accept': 144,\n",
       " 'brother': 372,\n",
       " 'lover': 1195,\n",
       " 'belovd': 313,\n",
       " 'rply': 1663,\n",
       " 'means': 1247,\n",
       " 'enemy': 689,\n",
       " 'any': 215,\n",
       " 'special': 1832,\n",
       " 'doing': 638,\n",
       " 'talking': 1919,\n",
       " 'think': 1969,\n",
       " 'being': 310,\n",
       " 'boy': 360,\n",
       " 'aft': 164,\n",
       " 'work': 2236,\n",
       " 'liao': 1144,\n",
       " 'horny': 965,\n",
       " 'naked': 1342,\n",
       " 'charged': 450,\n",
       " '150pm': 46,\n",
       " 'unsubscribe': 2081,\n",
       " 'feb': 752,\n",
       " 'valued': 2107,\n",
       " 'frnds': 813,\n",
       " 'comes': 495,\n",
       " 'married': 1231,\n",
       " 'luv': 1206,\n",
       " 'ignore': 997,\n",
       " 'polyphonic': 1526,\n",
       " '1000': 27,\n",
       " 'girl': 852,\n",
       " 'name': 1343,\n",
       " 'age': 170,\n",
       " 'eg': 678,\n",
       " 'join': 1064,\n",
       " 'gettin': 848,\n",
       " 'bit': 330,\n",
       " 'mo': 1289,\n",
       " 'ne': 1348,\n",
       " 'lovely': 1194,\n",
       " 'shall': 1734,\n",
       " 'slow': 1788,\n",
       " 'using': 2099,\n",
       " 'important': 1005,\n",
       " 'information': 1019,\n",
       " '02': 2,\n",
       " 'user': 2097,\n",
       " 'onto': 1421,\n",
       " 'http': 979,\n",
       " 'urawinner': 2088,\n",
       " 'fantastic': 742,\n",
       " 'surprise': 1904,\n",
       " 'awaiting': 268,\n",
       " 'hmm': 948,\n",
       " 'uncle': 2068,\n",
       " 'informed': 1020,\n",
       " 'paying': 1465,\n",
       " 'school': 1692,\n",
       " 'directly': 620,\n",
       " 'dad': 566,\n",
       " 'office': 1403,\n",
       " 'whats': 2190,\n",
       " 'msg': 1323,\n",
       " 'break': 365,\n",
       " 'rite': 1650,\n",
       " 'wan': 2146,\n",
       " 'watch': 2159,\n",
       " 'infernal': 1016,\n",
       " 'darren': 574,\n",
       " 'xy': 2261,\n",
       " 'dun': 663,\n",
       " 'sad': 1672,\n",
       " 'abt': 141,\n",
       " 'concentrate': 507,\n",
       " 'other': 1438,\n",
       " 'mode': 1296,\n",
       " 'press': 1545,\n",
       " 'gently': 843,\n",
       " 'remove': 1625,\n",
       " 'interesting': 1027,\n",
       " 'small': 1790,\n",
       " 'long': 1176,\n",
       " 'dear': 584,\n",
       " 'chechi': 455,\n",
       " 'living': 1163,\n",
       " 'liverpool': 1162,\n",
       " 'simple': 1765,\n",
       " 'shot': 1749,\n",
       " 'pass': 1461,\n",
       " 'mins': 1277,\n",
       " 'oic': 1406,\n",
       " 'sis': 1772,\n",
       " 'went': 2185,\n",
       " 'whether': 2195,\n",
       " 'near': 1349,\n",
       " 'camera': 412,\n",
       " 'awarded': 271,\n",
       " 'sipix': 1770,\n",
       " 'digital': 614,\n",
       " '09061221066': 23,\n",
       " 'fromm': 816,\n",
       " 'delivery': 599,\n",
       " 'within': 2219,\n",
       " '28': 65,\n",
       " 'days': 580,\n",
       " 'speak': 1831,\n",
       " 'customer': 561,\n",
       " 'machan': 1210,\n",
       " 'finished': 771,\n",
       " 'plenty': 1509,\n",
       " '800': 111,\n",
       " 'flights': 780,\n",
       " 'away': 272,\n",
       " 'b4': 274,\n",
       " 'al': 187,\n",
       " 'carlos': 425,\n",
       " 'sweet': 1906,\n",
       " 'usual': 2100,\n",
       " 'let': 1141,\n",
       " 'smoke': 1794,\n",
       " 'ass': 255,\n",
       " 'careful': 424,\n",
       " 'don': 640,\n",
       " 'drive': 653,\n",
       " 'thanks': 1953,\n",
       " 'picking': 1484,\n",
       " 'touch': 2023,\n",
       " 'skype': 1783,\n",
       " 'saying': 1686,\n",
       " 'welp': 2183,\n",
       " 'excuse': 725,\n",
       " 'too': 2017,\n",
       " 'spoke': 1840,\n",
       " 'experience': 728,\n",
       " 'hiya': 946,\n",
       " 'didn': 607,\n",
       " 'haven': 920,\n",
       " 'seen': 1708,\n",
       " 'heard': 927,\n",
       " 'itself': 1047,\n",
       " 'case': 429,\n",
       " 'sort': 1822,\n",
       " 'forget': 793,\n",
       " 'private': 1552,\n",
       " 'relation': 1619,\n",
       " 'iz': 1049,\n",
       " 'lot': 1188,\n",
       " 'feeling': 754,\n",
       " 'leaving': 1129,\n",
       " 'nope': 1378,\n",
       " 'bathing': 290,\n",
       " 'dog': 635,\n",
       " 'bathe': 289,\n",
       " 'looks': 1182,\n",
       " 'rain': 1581,\n",
       " 'pa': 1449,\n",
       " 'pain': 1452,\n",
       " 'de': 581,\n",
       " 'watching': 2160,\n",
       " 'alex': 190,\n",
       " 'pizza': 1493,\n",
       " 'wot': 2246,\n",
       " 'cud': 554,\n",
       " 'im': 1001,\n",
       " 'gona': 866,\n",
       " 'past': 1463,\n",
       " 'full': 821,\n",
       " 'waitin': 2140,\n",
       " 'pete': 1476,\n",
       " 'loved': 1193,\n",
       " 'gud': 886,\n",
       " 'pay': 1464,\n",
       " 'stock': 1860,\n",
       " 'comin': 496,\n",
       " 'phones': 1478,\n",
       " 'half': 900,\n",
       " 'price': 1550,\n",
       " 'rental': 1627,\n",
       " '12': 36,\n",
       " 'mths': 1327,\n",
       " 'cross': 550,\n",
       " 'ntwk': 1388,\n",
       " '100': 26,\n",
       " 'txts': 2058,\n",
       " 'mobileupd8': 1294,\n",
       " '08001950382': 11,\n",
       " 'call2optout': 401,\n",
       " 'details': 604,\n",
       " 'fri': 806,\n",
       " 'tom': 2010,\n",
       " 'chinese': 466,\n",
       " 'friends': 809,\n",
       " 'plan': 1496,\n",
       " 'valentines': 2103,\n",
       " 'url': 2092,\n",
       " 'order': 1433,\n",
       " 'ready': 1598,\n",
       " 'si': 1757,\n",
       " 'album': 188,\n",
       " 'quite': 1576,\n",
       " 'gd': 837,\n",
       " 'also': 198,\n",
       " 'comp': 499,\n",
       " 'winner': 2211,\n",
       " 'reward': 1644,\n",
       " 'valid': 2104,\n",
       " 'hours': 971,\n",
       " 'tea': 1929,\n",
       " 'missed': 1283,\n",
       " 'friendship': 810,\n",
       " 'o2': 1397,\n",
       " 'account': 148,\n",
       " 'credited': 548,\n",
       " 'credit': 547,\n",
       " 'valentine': 2102,\n",
       " 'paris': 1455,\n",
       " 'inc': 1008,\n",
       " 'flight': 779,\n",
       " 'hotel': 969,\n",
       " '200': 54,\n",
       " 'omw': 1413,\n",
       " 'last': 1112,\n",
       " 'difficult': 613,\n",
       " 'report': 1631,\n",
       " 'big': 321,\n",
       " 'made': 1213,\n",
       " 'previous': 1547,\n",
       " 'ni8': 1363,\n",
       " 'care': 422,\n",
       " 'swt': 1911,\n",
       " 'dreams': 649,\n",
       " 'internet': 1028,\n",
       " 'happen': 907,\n",
       " 'behave': 308,\n",
       " 'staying': 1856,\n",
       " 'til': 1991,\n",
       " 'huh': 981,\n",
       " 'ugh': 2063,\n",
       " 'fuck': 817,\n",
       " 'eve': 709,\n",
       " 'mad': 1212,\n",
       " 'woke': 2226,\n",
       " 'decimal': 590,\n",
       " 'points': 1522,\n",
       " 'places': 1495,\n",
       " 'chikku': 462,\n",
       " 'marriage': 1230,\n",
       " 'yet': 2275,\n",
       " 'forwarded': 796,\n",
       " 'alert': 189,\n",
       " 'matches': 1235,\n",
       " 'maid': 1216,\n",
       " 'murderer': 1334,\n",
       " 'murdered': 1333,\n",
       " 'th': 1949,\n",
       " 'january': 1054,\n",
       " 'closed': 479,\n",
       " 'including': 1012,\n",
       " 'post': 1534,\n",
       " 'understand': 2071,\n",
       " '2004': 57,\n",
       " 'statement': 1853,\n",
       " 'shows': 1755,\n",
       " '786': 109,\n",
       " 'unredeemed': 2078,\n",
       " 'bonus': 342,\n",
       " 'identifier': 993,\n",
       " '45239': 88,\n",
       " 'expires': 729,\n",
       " '06': 6,\n",
       " '05': 5,\n",
       " 'wasn': 2156,\n",
       " 'msgs': 1326,\n",
       " 'semester': 1714,\n",
       " 'haha': 896,\n",
       " 'noon': 1377,\n",
       " 'st': 1845,\n",
       " 'jordan': 1069,\n",
       " 'miss': 1282,\n",
       " 'celeb': 439,\n",
       " 'pics': 1485,\n",
       " 'pocketbabe': 1519,\n",
       " 'co': 482,\n",
       " 'uk': 2064,\n",
       " 'another': 209,\n",
       " 'frnd': 812,\n",
       " 'best': 315,\n",
       " 'wit': 2217,\n",
       " 'one': 1416,\n",
       " 'dude': 661,\n",
       " 'afraid': 163,\n",
       " 'planning': 1499,\n",
       " 'chennai': 461,\n",
       " 'gonna': 868,\n",
       " 'found': 797,\n",
       " 'eh': 680,\n",
       " 'net': 1353,\n",
       " 'cafe': 396,\n",
       " 'geeee': 840,\n",
       " 'crave': 543,\n",
       " 'none': 1376,\n",
       " 'must': 1336,\n",
       " 'indian': 1015,\n",
       " 'finally': 766,\n",
       " 'kerala': 1087,\n",
       " 'strike': 1871,\n",
       " 'against': 169,\n",
       " 'system': 1912,\n",
       " 'side': 1759,\n",
       " 'yr': 2285,\n",
       " 'draw': 647,\n",
       " 'gift': 851,\n",
       " 'voucher': 2129,\n",
       " 'receipt': 1606,\n",
       " 'correct': 527,\n",
       " 'ans': 210,\n",
       " '80062': 112,\n",
       " 'double': 644,\n",
       " 'hair': 898,\n",
       " 'dresser': 650,\n",
       " 'wun': 2252,\n",
       " 'cut': 562,\n",
       " 'short': 1746,\n",
       " 'look': 1178,\n",
       " 'bed': 302,\n",
       " 'sleep': 1785,\n",
       " 'lab': 1104,\n",
       " 'exactly': 719,\n",
       " 'could': 533,\n",
       " 'story': 1866,\n",
       " 'wish': 2214,\n",
       " 'kids': 1090,\n",
       " 'even': 710,\n",
       " 'colleagues': 487,\n",
       " 'problem': 1556,\n",
       " 'yijue': 2276,\n",
       " 'meet': 1254,\n",
       " 'run': 1668,\n",
       " 'water': 2161,\n",
       " 'paper': 1453,\n",
       " 'lost': 1187,\n",
       " 'ill': 1000,\n",
       " 'here': 937,\n",
       " 'never': 1359,\n",
       " 'knew': 1097,\n",
       " 'felt': 756,\n",
       " 'thatåõs': 1957,\n",
       " 'exam': 720,\n",
       " 'march': 1227,\n",
       " 'anyway': 222,\n",
       " 'guys': 891,\n",
       " 'movies': 1318,\n",
       " 'wylie': 2254,\n",
       " 'wishing': 2216,\n",
       " 'family': 739,\n",
       " 'merry': 1263,\n",
       " 'mas': 1233,\n",
       " 'happy': 913,\n",
       " 'year': 2268,\n",
       " 'advance': 161,\n",
       " 'plans': 1500,\n",
       " 'tried': 2035,\n",
       " 'played': 1502,\n",
       " 'original': 1435,\n",
       " 'yesterday': 2274,\n",
       " 'true': 2038,\n",
       " 'tv': 2051,\n",
       " 'start': 1849,\n",
       " 'leh': 1135,\n",
       " 'busy': 385,\n",
       " 'life': 1147,\n",
       " 'takes': 1916,\n",
       " 'turns': 2050,\n",
       " 'sit': 1774,\n",
       " 'try': 2042,\n",
       " 'hold': 952,\n",
       " 'crazy': 544,\n",
       " 'birla': 327,\n",
       " 'soft': 1802,\n",
       " 'followed': 784,\n",
       " 'gay': 835,\n",
       " 'men': 1261,\n",
       " 'cup': 556,\n",
       " '87239': 128,\n",
       " 'end': 685,\n",
       " 'business': 384,\n",
       " 'diff': 611,\n",
       " 'shop': 1744,\n",
       " 'jay': 1056,\n",
       " 'wants': 2152,\n",
       " 'wake': 2142,\n",
       " 'checked': 457,\n",
       " 'stuff': 1877,\n",
       " 'saw': 1684,\n",
       " 'available': 265,\n",
       " 'mail': 1217,\n",
       " 'searching': 1697,\n",
       " 'nothing': 1383,\n",
       " 'making': 1222,\n",
       " 'everybody': 714,\n",
       " 'others': 1439,\n",
       " 'probably': 1555,\n",
       " 'live': 1161,\n",
       " 'lead': 1124,\n",
       " 'bus': 383,\n",
       " 'aight': 178,\n",
       " 'open': 1424,\n",
       " 'ac': 142,\n",
       " 'energy': 690,\n",
       " 'high': 941,\n",
       " 'may': 1241,\n",
       " '2day': 66,\n",
       " 'strong': 1872,\n",
       " 'bags': 281,\n",
       " 'john': 1063,\n",
       " 'own': 1447,\n",
       " 'sha': 1733,\n",
       " 'mistake': 1285,\n",
       " 'contact': 518,\n",
       " 'settled': 1728,\n",
       " 'many': 1225,\n",
       " 'returns': 1641,\n",
       " 'heart': 928,\n",
       " 'birthday': 328,\n",
       " 'asking': 252,\n",
       " 'train': 2030,\n",
       " 'mine': 1274,\n",
       " 'laptop': 1109,\n",
       " 'btw': 376,\n",
       " 'whole': 2200,\n",
       " 'contacted': 519,\n",
       " 'land': 1107,\n",
       " ...}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "vect_tfidf.get_feature_names()\n",
    "vect_tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    vect_tfidf = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "    X_test_vectorized = vect_tfidf.transform(X_test)\n",
    "    \n",
    "    model = MultinomialNB(alpha=0.1)\n",
    "    model.fit(X_train_vectorized,y_train)\n",
    "    prediction = model.predict(X_test_vectorized)\n",
    "    \n",
    "    return roc_auc_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905660377358491"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What is the average length of documents (number of characters) for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average length not spam, average length spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.023627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138.866131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               len\n",
       "target            \n",
       "0        71.023627\n",
       "1       138.866131"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['len'] = [len(doc) for doc in spam_data['text']]\n",
    "spam_data.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():\n",
    "    spam_data['len'] = [len(doc) for doc in spam_data['text']]\n",
    "    grouped = spam_data.groupby('target').mean()\n",
    "    return (grouped.loc[0,'len'],grouped.loc[1,'len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to help you combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix and an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46095 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "X_test_vectorized = vect_tfidf.transform(X_test)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 130,\n",
       " 66,\n",
       " 146,\n",
       " 124,\n",
       " 23,\n",
       " 128,\n",
       " 35,\n",
       " 32,\n",
       " 25,\n",
       " 24,\n",
       " 22,\n",
       " 160,\n",
       " 28,\n",
       " 28,\n",
       " 133,\n",
       " 125,\n",
       " 27,\n",
       " 35,\n",
       " 153,\n",
       " 162,\n",
       " 64,\n",
       " 139,\n",
       " 54,\n",
       " 28,\n",
       " 128,\n",
       " 30,\n",
       " 46,\n",
       " 55,\n",
       " 33,\n",
       " 38,\n",
       " 64,\n",
       " 26,\n",
       " 45,\n",
       " 34,\n",
       " 104,\n",
       " 50,\n",
       " 154,\n",
       " 30,\n",
       " 51,\n",
       " 24,\n",
       " 39,\n",
       " 28,\n",
       " 25,\n",
       " 81,\n",
       " 145,\n",
       " 50,\n",
       " 29,\n",
       " 17,\n",
       " 36,\n",
       " 32,\n",
       " 122,\n",
       " 64,\n",
       " 32,\n",
       " 148,\n",
       " 43,\n",
       " 408,\n",
       " 47,\n",
       " 160,\n",
       " 22,\n",
       " 49,\n",
       " 168,\n",
       " 13,\n",
       " 29,\n",
       " 36,\n",
       " 74,\n",
       " 17,\n",
       " 32,\n",
       " 74,\n",
       " 157,\n",
       " 114,\n",
       " 155,\n",
       " 35,\n",
       " 155,\n",
       " 27,\n",
       " 35,\n",
       " 43,\n",
       " 109,\n",
       " 149,\n",
       " 162,\n",
       " 109,\n",
       " 79,\n",
       " 47,\n",
       " 77,\n",
       " 35,\n",
       " 70,\n",
       " 51,\n",
       " 152,\n",
       " 143,\n",
       " 141,\n",
       " 55,\n",
       " 27,\n",
       " 101,\n",
       " 39,\n",
       " 39,\n",
       " 52,\n",
       " 65,\n",
       " 40,\n",
       " 35,\n",
       " 38,\n",
       " 29,\n",
       " 149,\n",
       " 156,\n",
       " 32,\n",
       " 147,\n",
       " 38,\n",
       " 35,\n",
       " 81,\n",
       " 95,\n",
       " 64,\n",
       " 109,\n",
       " 156,\n",
       " 23,\n",
       " 23,\n",
       " 152,\n",
       " 67,\n",
       " 22,\n",
       " 121,\n",
       " 169,\n",
       " 158,\n",
       " 104,\n",
       " 119,\n",
       " 23,\n",
       " 24,\n",
       " 28,\n",
       " 163,\n",
       " 86,\n",
       " 34,\n",
       " 72,\n",
       " 77,\n",
       " 81,\n",
       " 141,\n",
       " 44,\n",
       " 50,\n",
       " 39,\n",
       " 45,\n",
       " 155,\n",
       " 97,\n",
       " 37,\n",
       " 106,\n",
       " 30,\n",
       " 34,\n",
       " 13,\n",
       " 33,\n",
       " 158,\n",
       " 27,\n",
       " 127,\n",
       " 39,\n",
       " 32,\n",
       " 69,\n",
       " 74,\n",
       " 33,\n",
       " 26,\n",
       " 62,\n",
       " 126,\n",
       " 169,\n",
       " 34,\n",
       " 111,\n",
       " 120,\n",
       " 27,\n",
       " 96,\n",
       " 24,\n",
       " 27,\n",
       " 56,\n",
       " 36,\n",
       " 26,\n",
       " 107,\n",
       " 148,\n",
       " 53,\n",
       " 145,\n",
       " 104,\n",
       " 69,\n",
       " 72,\n",
       " 24,\n",
       " 136,\n",
       " 158,\n",
       " 49,\n",
       " 44,\n",
       " 163,\n",
       " 130,\n",
       " 134,\n",
       " 38,\n",
       " 26,\n",
       " 67,\n",
       " 82,\n",
       " 53,\n",
       " 49,\n",
       " 36,\n",
       " 43,\n",
       " 73,\n",
       " 12,\n",
       " 32,\n",
       " 61,\n",
       " 126,\n",
       " 144,\n",
       " 173,\n",
       " 31,\n",
       " 27,\n",
       " 42,\n",
       " 166,\n",
       " 153,\n",
       " 34,\n",
       " 71,\n",
       " 54,\n",
       " 67,\n",
       " 56,\n",
       " 155,\n",
       " 28,\n",
       " 74,\n",
       " 46,\n",
       " 24,\n",
       " 33,\n",
       " 51,\n",
       " 158,\n",
       " 276,\n",
       " 60,\n",
       " 146,\n",
       " 26,\n",
       " 110,\n",
       " 75,\n",
       " 24,\n",
       " 41,\n",
       " 41,\n",
       " 154,\n",
       " 79,\n",
       " 41,\n",
       " 72,\n",
       " 33,\n",
       " 164,\n",
       " 60,\n",
       " 35,\n",
       " 46,\n",
       " 30,\n",
       " 46,\n",
       " 56,\n",
       " 75,\n",
       " 42,\n",
       " 48,\n",
       " 31,\n",
       " 69,\n",
       " 85,\n",
       " 9,\n",
       " 40,\n",
       " 41,\n",
       " 40,\n",
       " 18,\n",
       " 149,\n",
       " 30,\n",
       " 104,\n",
       " 29,\n",
       " 85,\n",
       " 43,\n",
       " 126,\n",
       " 87,\n",
       " 56,\n",
       " 66,\n",
       " 27,\n",
       " 61,\n",
       " 115,\n",
       " 51,\n",
       " 30,\n",
       " 154,\n",
       " 236,\n",
       " 94,\n",
       " 81,\n",
       " 84,\n",
       " 48,\n",
       " 39,\n",
       " 30,\n",
       " 101,\n",
       " 140,\n",
       " 67,\n",
       " 156,\n",
       " 124,\n",
       " 29,\n",
       " 108,\n",
       " 136,\n",
       " 205,\n",
       " 32,\n",
       " 38,\n",
       " 34,\n",
       " 157,\n",
       " 160,\n",
       " 133,\n",
       " 68,\n",
       " 26,\n",
       " 159,\n",
       " 109,\n",
       " 115,\n",
       " 50,\n",
       " 152,\n",
       " 7,\n",
       " 107,\n",
       " 70,\n",
       " 72,\n",
       " 37,\n",
       " 33,\n",
       " 28,\n",
       " 66,\n",
       " 129,\n",
       " 152,\n",
       " 94,\n",
       " 33,\n",
       " 159,\n",
       " 96,\n",
       " 58,\n",
       " 17,\n",
       " 242,\n",
       " 41,\n",
       " 101,\n",
       " 148,\n",
       " 42,\n",
       " 36,\n",
       " 158,\n",
       " 18,\n",
       " 151,\n",
       " 156,\n",
       " 96,\n",
       " 40,\n",
       " 120,\n",
       " 35,\n",
       " 138,\n",
       " 26,\n",
       " 47,\n",
       " 31,\n",
       " 36,\n",
       " 154,\n",
       " 163,\n",
       " 72,\n",
       " 46,\n",
       " 38,\n",
       " 87,\n",
       " 53,\n",
       " 83,\n",
       " 45,\n",
       " 37,\n",
       " 127,\n",
       " 45,\n",
       " 73,\n",
       " 30,\n",
       " 55,\n",
       " 25,\n",
       " 47,\n",
       " 159,\n",
       " 27,\n",
       " 141,\n",
       " 67,\n",
       " 37,\n",
       " 39,\n",
       " 126,\n",
       " 42,\n",
       " 84,\n",
       " 89,\n",
       " 126,\n",
       " 80,\n",
       " 67,\n",
       " 161,\n",
       " 145,\n",
       " 34,\n",
       " 30,\n",
       " 10,\n",
       " 51,\n",
       " 90,\n",
       " 156,\n",
       " 43,\n",
       " 104,\n",
       " 263,\n",
       " 49,\n",
       " 159,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 38,\n",
       " 88,\n",
       " 29,\n",
       " 30,\n",
       " 28,\n",
       " 24,\n",
       " 155,\n",
       " 15,\n",
       " 54,\n",
       " 60,\n",
       " 153,\n",
       " 22,\n",
       " 66,\n",
       " 43,\n",
       " 47,\n",
       " 139,\n",
       " 75,\n",
       " 152,\n",
       " 153,\n",
       " 53,\n",
       " 55,\n",
       " 32,\n",
       " 91,\n",
       " 31,\n",
       " 27,\n",
       " 35,\n",
       " 95,\n",
       " 141,\n",
       " 44,\n",
       " 162,\n",
       " 56,\n",
       " 47,\n",
       " 34,\n",
       " 141,\n",
       " 128,\n",
       " 42,\n",
       " 43,\n",
       " 132,\n",
       " 38,\n",
       " 35,\n",
       " 87,\n",
       " 27,\n",
       " 172,\n",
       " 134,\n",
       " 327,\n",
       " 28,\n",
       " 23,\n",
       " 52,\n",
       " 116,\n",
       " 143,\n",
       " 74,\n",
       " 73,\n",
       " 38,\n",
       " 63,\n",
       " 44,\n",
       " 38,\n",
       " 34,\n",
       " 49,\n",
       " 158,\n",
       " 81,\n",
       " 59,\n",
       " 43,\n",
       " 95,\n",
       " 80,\n",
       " 96,\n",
       " 86,\n",
       " 30,\n",
       " 52,\n",
       " 113,\n",
       " 62,\n",
       " 141,\n",
       " 30,\n",
       " 26,\n",
       " 25,\n",
       " 53,\n",
       " 129,\n",
       " 71,\n",
       " 153,\n",
       " 44,\n",
       " 22,\n",
       " 160,\n",
       " 36,\n",
       " 140,\n",
       " 152,\n",
       " 156,\n",
       " 27,\n",
       " 144,\n",
       " 46,\n",
       " 23,\n",
       " 140,\n",
       " 27,\n",
       " 28,\n",
       " 42,\n",
       " 40,\n",
       " 45,\n",
       " 5,\n",
       " 76,\n",
       " 99,\n",
       " 89,\n",
       " 30,\n",
       " 160,\n",
       " 110,\n",
       " 75,\n",
       " 4,\n",
       " 143,\n",
       " 84,\n",
       " 74,\n",
       " 175,\n",
       " 24,\n",
       " 95,\n",
       " 80,\n",
       " 162,\n",
       " 26,\n",
       " 291,\n",
       " 47,\n",
       " 34,\n",
       " 16,\n",
       " 50,\n",
       " 86,\n",
       " 41,\n",
       " 34,\n",
       " 375,\n",
       " 29,\n",
       " 154,\n",
       " 24,\n",
       " 40,\n",
       " 138,\n",
       " 47,\n",
       " 32,\n",
       " 133,\n",
       " 28,\n",
       " 131,\n",
       " 49,\n",
       " 53,\n",
       " 32,\n",
       " 166,\n",
       " 5,\n",
       " 61,\n",
       " 95,\n",
       " 4,\n",
       " 148,\n",
       " 153,\n",
       " 28,\n",
       " 22,\n",
       " 31,\n",
       " 149,\n",
       " 142,\n",
       " 22,\n",
       " 46,\n",
       " 61,\n",
       " 149,\n",
       " 87,\n",
       " 151,\n",
       " 121,\n",
       " 32,\n",
       " 11,\n",
       " 58,\n",
       " 107,\n",
       " 60,\n",
       " 150,\n",
       " 48,\n",
       " 106,\n",
       " 17,\n",
       " 27,\n",
       " 45,\n",
       " 24,\n",
       " 95,\n",
       " 143,\n",
       " 147,\n",
       " 36,\n",
       " 150,\n",
       " 31,\n",
       " 36,\n",
       " 51,\n",
       " 159,\n",
       " 35,\n",
       " 29,\n",
       " 34,\n",
       " 114,\n",
       " 28,\n",
       " 22,\n",
       " 38,\n",
       " 153,\n",
       " 156,\n",
       " 28,\n",
       " 156,\n",
       " 39,\n",
       " 159,\n",
       " 87,\n",
       " 104,\n",
       " 160,\n",
       " 127,\n",
       " 71,\n",
       " 46,\n",
       " 150,\n",
       " 27,\n",
       " 30,\n",
       " 13,\n",
       " 75,\n",
       " 155,\n",
       " 44,\n",
       " 137,\n",
       " 60,\n",
       " 95,\n",
       " 114,\n",
       " 139,\n",
       " 142,\n",
       " 153,\n",
       " 36,\n",
       " 35,\n",
       " 56,\n",
       " 155,\n",
       " 96,\n",
       " 148,\n",
       " 39,\n",
       " 38,\n",
       " 128,\n",
       " 26,\n",
       " 145,\n",
       " 142,\n",
       " 24,\n",
       " 150,\n",
       " 49,\n",
       " 158,\n",
       " 47,\n",
       " 40,\n",
       " 30,\n",
       " 47,\n",
       " 62,\n",
       " 46,\n",
       " 69,\n",
       " 55,\n",
       " 137,\n",
       " 146,\n",
       " 117,\n",
       " 133,\n",
       " 78,\n",
       " 56,\n",
       " 38,\n",
       " 155,\n",
       " 37,\n",
       " 37,\n",
       " 40,\n",
       " 132,\n",
       " 80,\n",
       " 124,\n",
       " 47,\n",
       " 38,\n",
       " 59,\n",
       " 226,\n",
       " 71,\n",
       " 148,\n",
       " 21,\n",
       " 22,\n",
       " 30,\n",
       " 159,\n",
       " 31,\n",
       " 31,\n",
       " 14,\n",
       " 155,\n",
       " 26,\n",
       " 27,\n",
       " 87,\n",
       " 80,\n",
       " 36,\n",
       " 31,\n",
       " 134,\n",
       " 166,\n",
       " 141,\n",
       " 156,\n",
       " 36,\n",
       " 55,\n",
       " 26,\n",
       " 37,\n",
       " 146,\n",
       " 7,\n",
       " 62,\n",
       " 58,\n",
       " 76,\n",
       " 36,\n",
       " 116,\n",
       " 68,\n",
       " 139,\n",
       " 67,\n",
       " 160,\n",
       " 39,\n",
       " 134,\n",
       " 101,\n",
       " 57,\n",
       " 53,\n",
       " 30,\n",
       " 72,\n",
       " 132,\n",
       " 161,\n",
       " 108,\n",
       " 26,\n",
       " 179,\n",
       " 41,\n",
       " 115,\n",
       " 46,\n",
       " 69,\n",
       " 35,\n",
       " 30,\n",
       " 116,\n",
       " 156,\n",
       " 96,\n",
       " 55,\n",
       " 108,\n",
       " 27,\n",
       " 42,\n",
       " 149,\n",
       " 70,\n",
       " 26,\n",
       " 72,\n",
       " 159,\n",
       " 162,\n",
       " 65,\n",
       " 42,\n",
       " 31,\n",
       " 244,\n",
       " 72,\n",
       " 33,\n",
       " 34,\n",
       " 30,\n",
       " 32,\n",
       " 52,\n",
       " 44,\n",
       " 45,\n",
       " 50,\n",
       " 156,\n",
       " 85,\n",
       " 46,\n",
       " 35,\n",
       " 45,\n",
       " 158,\n",
       " 168,\n",
       " 81,\n",
       " 41,\n",
       " 40,\n",
       " 23,\n",
       " 50,\n",
       " 140,\n",
       " 43,\n",
       " 30,\n",
       " 48,\n",
       " 18,\n",
       " 149,\n",
       " 28,\n",
       " 132,\n",
       " 46,\n",
       " 40,\n",
       " 193,\n",
       " 85,\n",
       " 44,\n",
       " 126,\n",
       " 48,\n",
       " 7,\n",
       " 23,\n",
       " 162,\n",
       " 40,\n",
       " 143,\n",
       " 26,\n",
       " 38,\n",
       " 42,\n",
       " 161,\n",
       " 24,\n",
       " 32,\n",
       " 54,\n",
       " 53,\n",
       " 42,\n",
       " 132,\n",
       " 111,\n",
       " 158,\n",
       " 136,\n",
       " 71,\n",
       " 44,\n",
       " 43,\n",
       " 29,\n",
       " 41,\n",
       " 91,\n",
       " 156,\n",
       " 33,\n",
       " 155,\n",
       " 135,\n",
       " 237,\n",
       " 155,\n",
       " 159,\n",
       " 136,\n",
       " 28,\n",
       " 96,\n",
       " 169,\n",
       " 16,\n",
       " 74,\n",
       " 137,\n",
       " 91,\n",
       " 41,\n",
       " 76,\n",
       " 107,\n",
       " 3,\n",
       " 45,\n",
       " 147,\n",
       " 61,\n",
       " 136,\n",
       " 29,\n",
       " 39,\n",
       " 118,\n",
       " 160,\n",
       " 142,\n",
       " 55,\n",
       " 129,\n",
       " 25,\n",
       " 139,\n",
       " 25,\n",
       " 22,\n",
       " 149,\n",
       " 31,\n",
       " 35,\n",
       " 32,\n",
       " 26,\n",
       " 58,\n",
       " 147,\n",
       " 146,\n",
       " 114,\n",
       " 31,\n",
       " 22,\n",
       " 98,\n",
       " 65,\n",
       " 79,\n",
       " 82,\n",
       " 327,\n",
       " 162,\n",
       " 88,\n",
       " 25,\n",
       " 37,\n",
       " 30,\n",
       " 33,\n",
       " 158,\n",
       " 110,\n",
       " 92,\n",
       " 142,\n",
       " 106,\n",
       " 65,\n",
       " 79,\n",
       " 152,\n",
       " 23,\n",
       " 146,\n",
       " 28,\n",
       " 295,\n",
       " 31,\n",
       " 21,\n",
       " 98,\n",
       " 68,\n",
       " 137,\n",
       " 35,\n",
       " 123,\n",
       " 72,\n",
       " 76,\n",
       " 160,\n",
       " 152,\n",
       " 27,\n",
       " 74,\n",
       " 42,\n",
       " 131,\n",
       " 77,\n",
       " 48,\n",
       " 33,\n",
       " 121,\n",
       " 152,\n",
       " 11,\n",
       " 15,\n",
       " 159,\n",
       " 35,\n",
       " 35,\n",
       " 165,\n",
       " 31,\n",
       " 34,\n",
       " 127,\n",
       " 115,\n",
       " 91,\n",
       " 32,\n",
       " 52,\n",
       " 164,\n",
       " 49,\n",
       " 95,\n",
       " 154,\n",
       " 119,\n",
       " 70,\n",
       " 69,\n",
       " 57,\n",
       " 87,\n",
       " 147,\n",
       " 55,\n",
       " 45,\n",
       " 72,\n",
       " 122,\n",
       " 112,\n",
       " 31,\n",
       " 67,\n",
       " 71,\n",
       " 66,\n",
       " 28,\n",
       " 126,\n",
       " 24,\n",
       " 62,\n",
       " 39,\n",
       " 48,\n",
       " 49,\n",
       " 25,\n",
       " 160,\n",
       " 44,\n",
       " 121,\n",
       " 81,\n",
       " 60,\n",
       " 34,\n",
       " 17,\n",
       " 57,\n",
       " 24,\n",
       " 162,\n",
       " 140,\n",
       " 302,\n",
       " 52,\n",
       " 143,\n",
       " 45,\n",
       " 25,\n",
       " 69,\n",
       " 13,\n",
       " 31,\n",
       " 137,\n",
       " 14,\n",
       " 81,\n",
       " 40,\n",
       " 34,\n",
       " 21,\n",
       " 34,\n",
       " 66,\n",
       " 42,\n",
       " 162,\n",
       " 122,\n",
       " 37,\n",
       " 46,\n",
       " 135,\n",
       " 121,\n",
       " 120,\n",
       " 42,\n",
       " 160,\n",
       " 25,\n",
       " 133,\n",
       " 79,\n",
       " 23,\n",
       " 58,\n",
       " 163,\n",
       " 24,\n",
       " 101,\n",
       " 52,\n",
       " 49,\n",
       " 107,\n",
       " 70,\n",
       " 53,\n",
       " 26,\n",
       " 145,\n",
       " 174,\n",
       " 23,\n",
       " 80,\n",
       " 65,\n",
       " 55,\n",
       " 156,\n",
       " 26,\n",
       " 162,\n",
       " 36,\n",
       " 158,\n",
       " 97,\n",
       " 150,\n",
       " 46,\n",
       " 57,\n",
       " 37,\n",
       " 283,\n",
       " 76,\n",
       " 27,\n",
       " 154,\n",
       " 93,\n",
       " 126,\n",
       " 30,\n",
       " 94,\n",
       " 27,\n",
       " 160,\n",
       " 148,\n",
       " 141,\n",
       " 104,\n",
       " 49,\n",
       " 142,\n",
       " 29,\n",
       " 157,\n",
       " 25,\n",
       " 103,\n",
       " 128,\n",
       " 60,\n",
       " 154,\n",
       " 157,\n",
       " 329,\n",
       " 33,\n",
       " 40,\n",
       " 154,\n",
       " 63,\n",
       " 87,\n",
       " 40,\n",
       " 121,\n",
       " 26,\n",
       " 127,\n",
       " 164,\n",
       " 22,\n",
       " 26,\n",
       " 51,\n",
       " 70,\n",
       " 37,\n",
       " 47,\n",
       " 42,\n",
       " 46,\n",
       " 58,\n",
       " 148,\n",
       " 85,\n",
       " 114,\n",
       " 54,\n",
       " 10,\n",
       " 63,\n",
       " 34,\n",
       " 25,\n",
       " 29,\n",
       " 154,\n",
       " ...]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len = [len(doc) for doc in X_train]\n",
    "doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1469 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50274 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_feature(X_train_vectorized,doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_seven():\n",
    "    def add_feature(X, feature_to_add):\n",
    "        \"\"\"\n",
    "        Returns sparse feature matrix with added feature.\n",
    "        feature_to_add can also be a list of features.\n",
    "        \"\"\"\n",
    "        from scipy.sparse import csr_matrix, hstack\n",
    "        return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "    \n",
    "    vect_tfidf = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "    X_test_vectorized = vect_tfidf.transform(X_test)    \n",
    "    X_train_vectorized = add_feature(X_train_vectorized,[len(doc) for doc in X_train])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized,[len(doc) for doc in X_test])\n",
    "    \n",
    "    model = SVC(C=10000).fit(X_train_vectorized,y_train)\n",
    "    prediction = model.predict(X_test_vectorized)\n",
    "    return roc_auc_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661689557407943"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the average number of digits per document for not spam and spam documents?\n",
    "\n",
    "*This function should return a tuple (average # digits not spam, average # digits spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>num_digit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.023627</td>\n",
       "      <td>0.299275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138.866131</td>\n",
       "      <td>15.759036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               len  num_digit\n",
       "target                       \n",
       "0        71.023627   0.299275\n",
       "1       138.866131  15.759036"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumDigit(string):\n",
    "    return sum(c.isdigit() for c in string)\n",
    "\n",
    "spam_data['num_digit'] = [getNumDigit(doc) for doc in spam_data['text']]\n",
    "spam_data.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eight():\n",
    "    spam_data['num_digit'] = [sum(c.isdigit() for c in doc) for doc in spam_data['text']]\n",
    "    grouped = spam_data.groupby('target').mean()\n",
    "    return (grouped.loc[0,'num_digit'],grouped.loc[1,'num_digit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function should return the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_nine():\n",
    "    def add_feature(X, feature_to_add):\n",
    "        \"\"\"\n",
    "        Returns sparse feature matrix with added feature.\n",
    "        feature_to_add can also be a list of features.\n",
    "        \"\"\"\n",
    "        from scipy.sparse import csr_matrix, hstack\n",
    "        return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "    \n",
    "    vect_tfidf = TfidfVectorizer(min_df=5,ngram_range=(1,3)).fit(X_train)\n",
    "    X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "    X_test_vectorized = vect_tfidf.transform(X_test)    \n",
    "    X_train_vectorized = add_feature(X_train_vectorized,[len(doc) for doc in X_train])\n",
    "    X_train_vectorized = add_feature(X_train_vectorized,[sum(c.isdigit() for c in doc) for doc in X_train])\n",
    "    X_test_vectorized = add_feature(X_test_vectorized,[len(doc) for doc in X_test])    \n",
    "    X_test_vectorized = add_feature(X_test_vectorized,[sum(c.isdigit() for c in doc) for doc in X_test])\n",
    "    \n",
    "    model = LogisticRegression(C=100).fit(X_train_vectorized,y_train)\n",
    "    #model = LogisticRegression(C=100,max_iter=2000).fit(X_train_vectorized,y_train)\n",
    "    prediction = model.predict(X_test_vectorized)\n",
    "    \n",
    "    return roc_auc_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\netis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9884028364593043"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "What is the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents?\n",
    "\n",
    "*Hint: Use `\\w` and `\\W` character classes*\n",
    "\n",
    "*This function should return a tuple (average # non-word characters not spam, average # non-word characters spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = 'hello %%#@this is'\n",
    "len(re.findall(r'\\W',text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>num_non_word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.023627</td>\n",
       "      <td>0.299275</td>\n",
       "      <td>17.291813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138.866131</td>\n",
       "      <td>15.759036</td>\n",
       "      <td>29.041499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               len  num_digit  num_non_word\n",
       "target                                     \n",
       "0        71.023627   0.299275     17.291813\n",
       "1       138.866131  15.759036     29.041499"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['num_non_word'] = [len(re.findall(r'\\W',doc)) for doc in spam_data['text']]\n",
    "grouped = spam_data.groupby('target').mean()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>len</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>num_non_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  len  \\\n",
       "0     Go until jurong point, crazy.. Available only ...       0  111   \n",
       "1                         Ok lar... Joking wif u oni...       0   29   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...       1  155   \n",
       "3     U dun say so early hor... U c already then say...       0   49   \n",
       "4     Nah I don't think he goes to usf, he lives aro...       0   61   \n",
       "...                                                 ...     ...  ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...       1  161   \n",
       "5568              Will Ì_ b going to esplanade fr home?       0   37   \n",
       "5569  Pity, * was in mood for that. So...any other s...       0   57   \n",
       "5570  The guy did some bitching but I acted like i'd...       0  125   \n",
       "5571                         Rofl. Its true to its name       0   26   \n",
       "\n",
       "      num_digit  num_non_word  \n",
       "0             0            28  \n",
       "1             0            11  \n",
       "2            25            33  \n",
       "3             0            16  \n",
       "4             0            14  \n",
       "...         ...           ...  \n",
       "5567         21            38  \n",
       "5568          0             8  \n",
       "5569          0            16  \n",
       "5570          0            26  \n",
       "5571          0             6  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_ten():\n",
    "    import re\n",
    "    spam_data['num_non_word'] = [len(re.findall(r'\\W',doc)) for doc in spam_data['text']]\n",
    "    grouped = spam_data.groupby('target').mean()\n",
    "    \n",
    "    return (grouped.loc[0,'num_non_word'],grouped.loc[1,'num_non_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n",
    "\n",
    "The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first.\n",
    "\n",
    "The three features that were added to the document term matrix should have the following names should they appear in the list of coefficients:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "\n",
    "*This function should return a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "\n",
    "vect_tfidf = CountVectorizer(min_df=5,ngram_range=(2,5),analyzer='char_wb').fit(X_train)\n",
    "X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "X_test_vectorized = vect_tfidf.transform(X_test) \n",
    "\n",
    "df_train = pd.DataFrame([[len(doc) for doc in X_train],\n",
    "         [sum(c.isdigit() for c in doc) for doc in X_train],\n",
    "         [len(re.findall(r'\\W',doc)) for doc in X_train]],index = ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "df_test = pd.DataFrame([[len(doc) for doc in X_test],\n",
    "     [sum(c.isdigit() for c in doc) for doc in X_test],\n",
    "     [len(re.findall(r'\\W',doc)) for doc in X_test]],index = ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "X_train_vectorized = add_feature(X_train_vectorized,df_train)\n",
    "X_test_vectorized = add_feature(X_test_vectorized,df_test)\n",
    "\n",
    "model = LogisticRegression(C=100,max_iter=2000).fit(X_train_vectorized,y_train)\n",
    "prediction = model.predict(X_test_vectorized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62264827, -0.60799874, -0.54677537, ...,  0.57898698,\n",
       "        0.61905366,  1.47659575])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = model.coef_[0]\n",
    "coeff.sort()\n",
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62264827, -0.60799874, -0.54677537, -0.54618682, -0.51165218,\n",
       "       -0.50735158, -0.50427451, -0.48591303, -0.47257103, -0.46728126])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47659575, 0.61905366, 0.57898698, 0.5777927 , 0.57013544,\n",
       "       0.56465334, 0.54183617, 0.54094161, 0.53615428])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff[:-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x16316 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 746763 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# (3,4179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[len(doc) for doc in X_train],\n",
    "             [sum(c.isdigit() for c in doc) for doc in X_train],\n",
    "             [len(re.findall(r'\\W',doc)) for doc in X_train]],index = ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "X_train_vectorized = add_feature(X_train_vectorized, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_eleven():\n",
    "    import re\n",
    "    \n",
    "    def add_feature(X, feature_to_add):\n",
    "        \"\"\"\n",
    "        Returns sparse feature matrix with added feature.\n",
    "        feature_to_add can also be a list of features.\n",
    "        \"\"\"\n",
    "        from scipy.sparse import csr_matrix, hstack\n",
    "        return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n",
    "    \n",
    "    vect_tfidf = CountVectorizer(min_df=5,ngram_range=(2,5),analyzer='char_wb').fit(X_train)\n",
    "    X_train_vectorized = vect_tfidf.transform(X_train)\n",
    "    X_test_vectorized = vect_tfidf.transform(X_test) \n",
    "    \n",
    "    df_train = pd.DataFrame([[len(doc) for doc in X_train],\n",
    "             [sum(c.isdigit() for c in doc) for doc in X_train],\n",
    "             [len(re.findall(r'\\W',doc)) for doc in X_train]],index = ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "    \n",
    "    df_test = pd.DataFrame([[len(doc) for doc in X_test],\n",
    "         [sum(c.isdigit() for c in doc) for doc in X_test],\n",
    "         [len(re.findall(r'\\W',doc)) for doc in X_test]],index = ['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "\n",
    "    X_train_vectorized = add_feature(X_train_vectorized,df_train)\n",
    "    X_test_vectorized = add_feature(X_test_vectorized,df_test)\n",
    "    \n",
    "    model = LogisticRegression(C=100).fit(X_train_vectorized,y_train)\n",
    "    prediction = model.predict(X_test_vectorized)\n",
    "    \n",
    "    coeff = model.coef_[0]\n",
    "    coeff.sort()\n",
    "    \n",
    "    return (roc_auc_score(prediction,y_test),coeff[:10],coeff[:-10:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\netis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9918774285317791,\n",
       " array([-1.53230334, -1.29749374, -0.91735616, -0.78474499, -0.77132418,\n",
       "        -0.72739922, -0.70493751, -0.69269104, -0.6864433 , -0.67967923]),\n",
       " array([1.57438201, 0.8085238 , 0.80723617, 0.77828125, 0.66791756,\n",
       "        0.66490494, 0.65877183, 0.63931452, 0.62672943]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
